{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer to E-Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Data quality measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality measures are used to assess the reliability and accuracy of a dataset. Ensuring high-quality data is essential for successful data mining and analysis. Various metrics and techniques are employed to evaluate different aspects of data quality, including completeness, accuracy, consistency, and timeliness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Quality Measure: Missing Values\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Missing values are a common issue in datasets and can impact the quality of analysis. One measure of data quality is assessing the percentage of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Feature1': [10, 15, None, 25, 20],\n",
    "    'Feature2': [5, None, 8, 12, 10],\n",
    "    'Feature3': [2, 4, 6, None, 8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate percentage of missing values in each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"Percentage of Missing Values:\")\n",
    "print(missing_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the dataset contains missing values represented as None. The code calculates the percentage of missing values in each column using the isnull() function and the sum() function in pandas.\n",
    "\n",
    "    Assessing data quality is crucial for making informed decisions during the data mining process. The example focuses on missing values, but other data quality measures could include checking for outliers, ensuring consistency in categorical values, and validating data against predefined business rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is a crucial step in the data preprocessing phase, aiming to identify and rectify errors, inconsistencies, and inaccuracies in the dataset. It involves tasks such as handling missing values, correcting data types, removing duplicates, and addressing outliers. Ensuring clean and reliable data is essential for obtaining meaningful and accurate results in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Cleaning Task: Handling Missing Values\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "One common data cleaning task is handling missing values. Depending on the nature of the missing data, strategies such as imputation or removal may be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Feature1': [10, 15, None, 25, 20],\n",
    "    'Feature2': [5, None, 8, 12, 10],\n",
    "    'Feature3': [2, 4, 6, None, 8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Handling missing values by imputation (using mean)\n",
    "df_filled = df.fillna(df.mean())\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(\"Cleaned Dataset:\")\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the dataset contains missing values represented as None. The code uses the fillna function in pandas to replace missing values with the mean value of each column.\n",
    "\n",
    "    Data cleaning is a critical step to ensure the quality and reliability of the dataset. While the example focuses on handling missing values, other data cleaning tasks might include removing duplicate records, correcting data types, and dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. Data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data integration is the process of combining data from different sources into a unified and coherent view. It involves handling inconsistencies, resolving conflicts, and creating a merged dataset that can be used for analysis and mining. Data integration is crucial when working with diverse datasets that may come from various databases, files, or platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data Integration Task: Merging Datasets\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "A common data integration task is merging datasets with a common key or identifier. This helps create a consolidated dataset that includes information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example datasets with a common key\n",
    "data1 = {'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']}\n",
    "data2 = {'ID': [2, 3, 4], 'Age': [25, 30, 22]}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Merge datasets on the common key 'ID'\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='outer')\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"Merged Dataset:\")\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, two datasets (df1 and df2) share a common key ('ID'). The merge function in pandas is used to merge these datasets based on the 'ID' column.\n",
    "\n",
    "    Data integration is essential for creating a comprehensive view of the information and relationships across different datasets. While the example focuses on merging datasets, other data integration tasks might include handling schema mismatches, resolving naming conflicts, and dealing with data from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a data preprocessing technique used to scale and transform numerical features in a dataset to a common scale. This process is essential when the features have different units or scales, as it ensures that each feature contributes proportionally to the analysis. Normalization typically involves transforming the data to a standard scale, such as between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Normalization in Python: Min-Max Scaling\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Min-Max Scaling is a common normalization method that transforms the values of a feature to a specific range, usually between 0 and 1. The formula for Min-Max Scaling is: Xnormalized=X−Xmin/Xmax−Xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset with numerical features\n",
    "data = {\n",
    "    'Feature1': [10, 20, 15, 25, 30],\n",
    "    'Feature2': [500, 1000, 750, 1250, 1500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max Scaling to normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Display the normalized dataset\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "print(\"Normalized Dataset:\")\n",
    "print(normalized_df)\n",
    "\n",
    "# Plot original and normalized datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original dataset\n",
    "ax1.scatter(df['Feature1'], df['Feature2'], c='blue', label='Original Data')\n",
    "ax1.set_title('Original Dataset')\n",
    "ax1.set_xlabel('Feature1')\n",
    "ax1.set_ylabel('Feature2')\n",
    "ax1.legend()\n",
    "\n",
    "# Normalized dataset\n",
    "ax2.scatter(normalized_df['Feature1'], normalized_df['Feature2'], c='red', label='Normalized Data')\n",
    "ax2.set_title('Normalized Dataset')\n",
    "ax2.set_xlabel('Normalized Feature1')\n",
    "ax2.set_ylabel('Normalized Feature2')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the MinMaxScaler from scikit-learn is used to normalize the numerical features in the dataset.\n",
    "\n",
    "    Normalization is crucial for certain machine learning algorithms, especially those sensitive to the scale of the input features (e.g., k-nearest neighbors or support vector machines). The Min-Max Scaling example demonstrates how to normalize numerical features in a datase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization is a data preprocessing technique that involves converting continuous numerical attributes into discrete categories or bins. This can be useful in situations where the continuous nature of the data is not necessary or when working with algorithms that perform better with categorical data. Discretization simplifies the data and can make it more suitable for certain types of analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Discretization in Python: Equal Width Binning\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Equal Width Binning is a simple discretization method that divides the range of continuous values into equal-width intervals or bins. Each interval represents a discrete category, and values within that range are assigned to the corresponding bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Example dataset with a continuous numerical feature\n",
    "data = {\n",
    "    'Age': [22, 35, 42, 28, 55, 48, 33, 40, 60, 70]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Equal Width Binning to discretize the 'Age' feature\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "discretized_data = discretizer.fit_transform(df)\n",
    "\n",
    "# Display the discretized dataset\n",
    "discretized_df = pd.DataFrame(discretized_data, columns=df.columns)\n",
    "print(\"Discretized Dataset:\")\n",
    "print(discretized_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the KBinsDiscretizer from scikit-learn is used to apply equal-width binning to the 'Age' feature, dividing it into three bins.\n",
    "\n",
    "    Discretization is valuable when working with algorithms that require categorical input or when simplifying the analysis of continuous data. The Equal Width Binning example demonstrates how to discretize a numerical feature into categorical bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. Data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data compression is a process of reducing the size or volume of data to save storage space and improve computational efficiency. In the context of data mining, compression techniques can be applied to handle large datasets more efficiently, speed up processing, and reduce the resource requirements for storage and transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Data Compression in Python: Run-Length Encoding\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Run-Length Encoding (RLE) is a simple data compression technique that represents consecutive repeated values as a single value and a count. It is particularly effective for datasets with long runs of identical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoding(data):\n",
    "    encoded_data = []\n",
    "    count = 1\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i - 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            encoded_data.append((data[i - 1], count))\n",
    "            count = 1\n",
    "\n",
    "    # Add the last run\n",
    "    encoded_data.append((data[-1], count))\n",
    "    return encoded_data\n",
    "\n",
    "# Example dataset with consecutive repeated values\n",
    "data = [1, 1, 1, 2, 3, 3, 3, 3, 4, 4]\n",
    "\n",
    "# Apply Run-Length Encoding to compress the data\n",
    "compressed_data = run_length_encoding(data)\n",
    "\n",
    "# Display the compressed data\n",
    "print(\"Compressed Data:\")\n",
    "print(compressed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the run_length_encoding function is used to compress a dataset with consecutive repeated values using Run-Length Encoding.\n",
    "\n",
    "    Data compression is beneficial for reducing storage requirements and speeding up data processing. While Run-Length Encoding is a basic example, there are more advanced compression techniques like Huffman coding or Lempel-Ziv compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4. Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is a data preprocessing technique that involves selecting a subset of data from a larger dataset. This subset, known as a sample, is representative of the overall population and is often used for various purposes, such as exploratory analysis, model training, or performance testing. Sampling can be particularly valuable when working with large datasets, as it allows for more efficient processing and analysis of a manageable subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Sampling in Python: Random Sampling\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Random Sampling is a common sampling technique where data points are selected randomly from the dataset, providing an unbiased representation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'ID': range(1, 101),\n",
    "    'Value': np.random.randint(1, 100, size=100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform random sampling to select a subset of data\n",
    "sample_size = 20\n",
    "random_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Display the randomly sampled data\n",
    "print(\"Randomly Sampled Data:\")\n",
    "print(random_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the sample function from pandas is used to perform random sampling on a dataset. The random_state parameter ensures reproducibility.\n",
    "\n",
    "    Sampling is essential when dealing with large datasets to make computations more manageable and to speed up the analysis process. Random Sampling is just one of many sampling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1. Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA) is a dimensionality reduction technique used to transform a dataset into a lower-dimensional space while retaining as much of the original variance as possible. PCA identifies the principal components, which are orthogonal vectors that capture the directions of maximum variance in the data. By projecting the data onto these components, PCA allows for a more concise representation of the dataset, making it easier to visualize and analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Principal Components Analysis in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Apply PCA for dimensionality reduction to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame with the principal components and target\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pc_df['target'] = target\n",
    "\n",
    "# Plot the data in the reduced dimensional space\n",
    "plt.figure(figsize=(8, 6))\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indices_to_keep = pc_df['target'] == target\n",
    "    plt.scatter(pc_df.loc[indices_to_keep, 'PC1'],\n",
    "                pc_df.loc[indices_to_keep, 'PC2'],\n",
    "                c=color,\n",
    "                label=target,\n",
    "                alpha=0.7)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Principal Components Analysis')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, PCA is applied to the Iris dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers. The code reduces the dimensionality to two principal components and visualizes the data in the reduced space.\n",
    "\n",
    "    PCA is widely used for feature reduction, noise reduction, and visualization. The example demonstrates how PCA can be applied to simplify and visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2. Attribute subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute subset selection, also known as feature selection, involves identifying and selecting a subset of relevant features from the original set of attributes in a dataset. The goal is to improve model performance, reduce dimensionality, and enhance interpretability by retaining only the most informative attributes. This process is crucial for addressing the \"curse of dimensionality\" and can lead to more efficient and accurate data mining models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Attribute Subset Selection in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply RandomForestClassifier for feature selection\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on importance\n",
    "sfm = SelectFromModel(clf, threshold=0.2)\n",
    "sfm.fit(X_train, y_train)\n",
    "\n",
    "# Transform datasets with selected features\n",
    "X_train_selected = sfm.transform(X_train)\n",
    "X_test_selected = sfm.transform(X_test)\n",
    "\n",
    "# Train and evaluate a classifier with the selected features\n",
    "clf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = clf_selected.predict(X_test_selected)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_selected)\n",
    "print(f'Accuracy with selected features: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a RandomForestClassifier is used to assess feature importance, and SelectFromModel is applied to select features based on their importance. The classifier is then trained and evaluated using only the selected features.\n",
    "\n",
    "    Attribute subset selection is crucial for improving model efficiency, reducing overfitting, and enhancing interpretability. The example demonstrates how to apply attribute subset selection using feature importance from a random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3. Nonlinear dimensionality reduction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear dimensionality reduction methods aim to capture complex relationships and structures in high-dimensional data that may not be well-represented by linear techniques like PCA. These methods transform data into a lower-dimensional space while preserving the inherent nonlinear relationships within the data. Nonlinear dimensionality reduction is particularly useful when dealing with datasets containing intricate patterns, clusters, or manifolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Nonlinear Dimensionality Reduction in Python: t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target = pd.Series(data=iris.target, name='target')\n",
    "\n",
    "# Apply t-SNE for nonlinear dimensionality reduction to 2 components\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_data = tsne.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame with the embedded data and target\n",
    "embedded_df = pd.DataFrame(data=embedded_data, columns=['Component 1', 'Component 2'])\n",
    "embedded_df['target'] = target\n",
    "\n",
    "# Plot the data in the reduced dimensional space\n",
    "plt.figure(figsize=(8, 6))\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indices_to_keep = embedded_df['target'] == target\n",
    "    plt.scatter(embedded_df.loc[indices_to_keep, 'Component 1'],\n",
    "                embedded_df.loc[indices_to_keep, 'Component 2'],\n",
    "                c=color,\n",
    "                label=target,\n",
    "                alpha=0.7)\n",
    "\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('t-SNE: Nonlinear Dimensionality Reduction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, t-SNE is applied to the Iris dataset, and the data is visualized in a two-dimensional space. t-SNE is known for preserving local similarities, making it effective for capturing intricate structures and patterns in the data.\n",
    "\n",
    "    Nonlinear dimensionality reduction methods, like t-SNE, are valuable for visualizing and understanding complex relationships in high-dimensional data. The example introduces how t-SNE can be used for nonlinear dimensionality reduction and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
