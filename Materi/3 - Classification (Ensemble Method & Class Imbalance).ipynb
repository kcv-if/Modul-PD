{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer to E-Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Decision tree induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree induction is a popular machine learning technique used for both classification and regression tasks. It's a tree-like model where each node represents a decision based on a feature, and each branch represents the outcome of that decision, leading to the next node. The leaves of the tree represent the final output or the decision.\n",
    "Decision Tree Induction:\n",
    "\n",
    "#### Decision tree induction involves the following steps:\n",
    "\n",
    "    - Selection of the Root Node: The algorithm selects the feature that best splits the data into subsets, considering criteria such as Gini impurity or information gain.\n",
    "\n",
    "    - Splitting the Nodes: The data is split into subsets based on the chosen feature.\n",
    "\n",
    "    - Recursive Process: Steps 1 and 2 are recursively applied to each subset until a stopping criterion is met, such as a specific depth or a minimum number of samples in a node.\n",
    "\n",
    "    - Assigning Labels: The final step involves assigning labels or values to the leaves of the tree, based on the majority class or average value in the leaf node.\n",
    "\n",
    "Decision trees are interpretable, easy to understand, and can handle both numerical and categorical data.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a real-world example using the famous Iris dataset, where the goal is to classify iris flowers into three species (setosa, versicolor, or virginica) based on features like sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules\n",
    "tree_rules = export_text(clf, feature_names=iris.feature_names)\n",
    "print(\"Decision Tree Rules:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the scikit-learn library to create a DecisionTreeClassifier, train it on the Iris dataset, make predictions, and evaluate the accuracy. The export_text function is used to display the decision tree rules in a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Attribute selection measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data mining and machine learning, attribute selection measures, also known as feature selection, refer to the process of choosing a subset of relevant features from a larger set of features. The goal is to improve the model's performance, reduce overfitting, and enhance interpretability. There are various attribute selection measures, each with its own criteria for evaluating the importance of features. Some common \n",
    "#### measures include:\n",
    "\n",
    "1. Information Gain: Measures how well a feature separates the data into classes.\n",
    "\n",
    "2. Gain Ratio: Similar to Information Gain but adjusts for the number of branches a node has.\n",
    "\n",
    "3. Gini Index: Measures the impurity of a set of examples, with lower values indicating better splits.\n",
    "\n",
    "4. Chi-square: Tests the independence between the feature and the class, helping to select features that are statistically significant.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the famous Iris dataset again and apply Information Gain as the attribute selection measure to choose the most relevant features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use Information Gain for feature selection\n",
    "selector = SelectKBest(mutual_info_classif, k=2)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the selected training data\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the selected test data\n",
    "predictions = clf.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy of the model\n",
    "accuracy = clf.score(X_test_selected, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules for the selected features\n",
    "selected_feature_names = [iris.feature_names[i] for i in selector.get_support(indices=True)]\n",
    "tree_rules = export_text(clf, feature_names=selected_feature_names)\n",
    "print(\"Decision Tree Rules for Selected Features:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use scikit-learn's SelectKBest with mutual information as the scoring function to select the top k features with the highest information gain. We then train a decision tree classifier on the selected features and evaluate its accuracy. This example demonstrates the practical application of attribute selection measures to enhance the performance of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. Tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree pruning is a technique used in decision tree algorithms to prevent overfitting. Overfitting occurs when a tree is too complex and captures noise in the training data, leading to poor generalization on new, unseen data. Pruning involves removing parts of the tree that do not provide significant predictive power. There are two main types of pruning:\n",
    "\n",
    "1. Pre-pruning (Early Stopping): This involves stopping the tree-building process early, before it becomes too complex. Common pre-pruning strategies include limiting the maximum depth of the tree, setting a minimum number of samples required to split a node, or requiring a minimum number of samples in a leaf.\n",
    "\n",
    "2. Post-pruning (Pruning After Tree Construction): This involves building the full tree and then removing nodes that do not contribute significantly to predictive accuracy. Post-pruning methods include cost-complexity pruning, where a hyperparameter (alpha) controls the trade-off between tree complexity and fit to the training data.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the Iris dataset again and apply post-pruning using cost-complexity pruning to a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for cost-complexity pruning\n",
    "param_grid = {'ccp_alpha': [0.001, 0.002, 0.003, 0.004, 0.005]}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameter\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best decision tree classifier\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy of the pruned tree\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display the decision tree rules for the pruned tree\n",
    "tree_rules = export_text(best_clf, feature_names=iris.feature_names)\n",
    "print(\"Decision Tree Rules for Pruned Tree:\\n\", tree_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use GridSearchCV to perform a search over a hyperparameter grid for the best alpha value (ccp_alpha) for cost-complexity pruning. The selected hyperparameter helps control the trade-off between tree complexity and fit to the training data, resulting in a pruned decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. Bayes’ theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory, named after Reverend Thomas Bayes. It provides a way to update our beliefs about a hypothesis based on new evidence. The theorem is expressed mathematically as:\n",
    "\n",
    "P(A∣B)=P(B∣A)⋅P(A)P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(A∣B) is the probability of event A occurring given that event B has occurred.\n",
    "    - P(B∣A) is the probability of event B occurring given that event A has occurred.\n",
    "    - P(A) and P(B) are the probabilities of events A and B occurring, respectively.\n",
    "\n",
    "Bayes' Theorem is widely used in statistics and machine learning for tasks such as classification, spam filtering, and medical diagnosis.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of spam email classification using the Naive Bayes algorithm. We'll use the famous \"Spambase\" dataset, which contains features based on the frequency of certain words and characters in emails, along with labels indicating whether an email is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the Spambase dataset (you may need to adjust the path)\n",
    "data = pd.read_csv('path_to_spambase_dataset/spambase.csv')\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes classifier (MultinomialNB for discrete features)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Naive Bayes algorithm (specifically, Multinomial Naive Bayes) to classify emails as spam or non-spam based on the frequency of certain words and characters. The CountVectorizer is used to convert the text data into numerical features suitable for the Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Naïve Bayesian classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayesian Classification is a probabilistic classification technique based on Bayes' Theorem with the \"naïve\" assumption of feature independence. Despite its simplicity and the independence assumption, Naïve Bayes classifiers often perform well in practice, especially for text classification and spam filtering.\n",
    "\n",
    "The \"naïve\" assumption implies that the presence (or absence) of a particular feature in a class is independent of the presence (or absence) of other features. This assumption simplifies the computation and makes it computationally efficient, even for datasets with a large number of features.\n",
    "\n",
    "The basic formula for Naïve Bayesian Classification is:\n",
    "\n",
    "P(y∣X)=P(X∣y)⋅P(y)P(X)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(y∣X) is the probability of class y given the features X.\n",
    "    - P(X∣y) is the probability of features X given class y.\n",
    "    - P(y) is the prior probability of class y.\n",
    "    - P(X) is the prior probability of features X.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of text classification using Naïve Bayes. We'll use the famous 20 Newsgroups dataset, which consists of newsgroup documents categorized into 20 different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Naïve Bayes classifier (MultinomialNB for discrete features)\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the TF-IDF transformed training data\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the TF-IDF transformed test data\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=newsgroups.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Multinomial Naive Bayes classifier on the TF-IDF (Term Frequency-Inverse Document Frequency) transformed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1. k-nearest-neighbor classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest-Neighbor (k-NN) algorithm is a simple and intuitive classification technique based on the idea that similar instances are likely to belong to the same class. In other words, it classifies a data point based on the majority class of its k nearest neighbors in the feature space. The distance metric (e.g., Euclidean distance) is commonly used to measure the similarity between instances.\n",
    "\n",
    "#### The key steps in the k-NN algorithm are:\n",
    "\n",
    "1. Choose a value for k: Decide on the number of neighbors (k) to consider when making a classification.\n",
    "\n",
    "2. Compute distances: Calculate the distance between the target instance and all instances in the training set.\n",
    "\n",
    "3. Identify k-nearest neighbors: Select the k instances with the smallest distances.\n",
    "\n",
    "4. Majority voting: Assign the class label based on the majority class among the k neighbors.\n",
    "\n",
    "k-NN is a lazy learner, meaning it doesn't build a model during the training phase. Instead, it stores the entire training dataset and performs computations at the time of prediction.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's use the famous Iris dataset and apply the k-NN algorithm for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier with k=3 (you can adjust the value of k)\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use scikit-learn to create a k-NN classifier with k=3. The classifier is trained on the Iris dataset, and predictions are made on the test data. The accuracy and classification report are then displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2. Case-based reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case-Based Reasoning is a problem-solving approach that relies on retrieving and adapting solutions from past experiences or cases. It operates on the principle that similar problems have similar solutions. CBR consists of four main steps:\n",
    "\n",
    "1. Retrieve: Identify similar cases from the case base (database of past experiences) based on the current problem.\n",
    "\n",
    "2. Reuse: Apply the solution from the retrieved case to the current problem. If an exact match is not found, adapt the solution to fit the current context.\n",
    "\n",
    "3. Revise: Evaluate the solution's success and, if necessary, revise the solution based on feedback or new information.\n",
    "\n",
    "4. Retain: Store the new case in the case base for future use.\n",
    "\n",
    "CBR is particularly useful in situations where traditional rule-based or model-based approaches may be challenging due to uncertainty, complexity, or changing environments.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of case-based reasoning for a recommendation system. We'll use the MovieLens dataset and recommend movies based on user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the MovieLens dataset (you may need to adjust the path)\n",
    "movies = pd.read_csv('path_to_movielens_dataset/movies.csv')\n",
    "ratings = pd.read_csv('path_to_movielens_dataset/ratings.csv')\n",
    "\n",
    "# Merge movies and ratings data\n",
    "movie_ratings = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Create a user-item matrix for collaborative filtering\n",
    "user_item_matrix = movie_ratings.pivot_table(index='userId', columns='title', values='rating', fill_value=0)\n",
    "\n",
    "# Use TF-IDF to convert movie titles into numerical features\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['title'])\n",
    "\n",
    "# Calculate cosine similarity between movie titles\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to get movie recommendations using CBR\n",
    "def get_movie_recommendations(movie_title):\n",
    "    movie_index = movies.index[movies['title'] == movie_title].tolist()[0]\n",
    "    cosine_scores = list(enumerate(cosine_sim[movie_index]))\n",
    "    cosine_scores = sorted(cosine_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_similar_movies = cosine_scores[1:6]  # Exclude the input movie itself\n",
    "\n",
    "    recommended_movies = []\n",
    "    for index, score in top_similar_movies:\n",
    "        recommended_movies.append(movies['title'].iloc[index])\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "# Example usage\n",
    "input_movie = \"The Dark Knight\"\n",
    "recommendations = get_movie_recommendations(input_movie)\n",
    "\n",
    "# Display the recommendations\n",
    "print(f\"Movies similar to '{input_movie}':\")\n",
    "for movie in recommendations:\n",
    "    print(\"-\", movie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use case-based reasoning to recommend movies based on the similarity of their titles. The TF-IDF vectorizer is used to convert movie titles into numerical features, and cosine similarity is calculated to measure the similarity between movies. The get_movie_recommendations function retrieves similar movies based on the input movie title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. The relationship is assumed to be linear, meaning that changes in the dependent variable are proportional to changes in the independent variable(s). The basic equation for a simple linear regression is:\n",
    "\n",
    "y=mx+b\n",
    "\n",
    "where:\n",
    "\n",
    "    - y is the dependent variable.\n",
    "    - x is the independent variable.\n",
    "    - m is the slope (the rate at which y changes with respect to x).\n",
    "    - b is the y-intercept (the value of y when x is 0).\n",
    "\n",
    "In multiple linear regression, the equation extends to include multiple independent variables:\n",
    "\n",
    "y=b0+b1x1+b2x2+…+bnxn\n",
    "\n",
    "where:\n",
    "\n",
    "    - b0​ is the y-intercept.\n",
    "    - b1,b2,…,bn​ are the coefficients for the independent variables x1,x2,…,xn.\n",
    "\n",
    "Linear regression is commonly used for predicting numeric values, such as predicting house prices based on square footage or predicting sales based on advertising spend.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of linear regression for predicting house prices using the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['target'])\n",
    "\n",
    "# Select a single feature (e.g., 'RM' - average number of rooms per dwelling)\n",
    "X_feature = X[['RM']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display the model coefficients and performance metrics\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.scatter(X_test, y_test, color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "plt.xlabel('Average Number of Rooms (RM)')\n",
    "plt.ylabel('House Price')\n",
    "plt.title('Linear Regression: House Price Prediction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the average number of rooms ('RM') as the independent variable to predict house prices. The linear regression model is trained on the Boston Housing dataset, and predictions are made on the test data. The performance of the model is evaluated using mean squared error and R-squared. The regression line is plotted to visualize the relationship between the average number of rooms and house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2. Perceptron: turning linear regression to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is a basic building block of artificial neural networks and serves as a simple binary classifier. It's an algorithm for supervised learning that takes a set of input features and produces an output (binary classification) based on a linear combination of those inputs. The perceptron is trained to learn the weights associated with each input feature, and it applies a step function to make a binary decision.\n",
    "\n",
    "In the context of turning linear regression into classification using a perceptron:\n",
    "\n",
    "1. Linear Regression:\n",
    "    In linear regression, the model predicts a continuous output. The linear regression equation is used to calculate a numerical value based on input features.\n",
    "\n",
    "2. Thresholding with Perceptron:\n",
    "    To convert this into a classification problem, a thresholding step is introduced. If the calculated numerical value is above a certain threshold, the perceptron outputs one class (e.g., 1), and if it's below the threshold, it outputs the other class (e.g., 0).\n",
    "\n",
    "3. Activation Function:\n",
    "    The step function used for thresholding is the activation function of the perceptron. Commonly used activation functions include the Heaviside step function or the sigmoid function.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of using a perceptron for binary classification using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Consider only the first two features for simplicity and binary classification\n",
    "X = X[:, :2]\n",
    "\n",
    "# Map iris classes to binary classes (setosa vs. non-setosa)\n",
    "y_binary = (y == 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a perceptron classifier\n",
    "perceptron = Perceptron()\n",
    "\n",
    "# Train the perceptron on the training data\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = perceptron.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_binary, cmap=plt.cm.Paired, edgecolors='k')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Perceptron: Iris Binary Classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the first two features of the Iris dataset for simplicity and perform binary classification to distinguish setosa from non-setosa flowers. The perceptron is trained on the training data and used to make predictions on the test data. The accuracy and classification report are then displayed, and the decision boundary is visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its name, logistic regression is a classification algorithm, not a regression one. It's particularly useful for binary classification problems, where the goal is to predict whether an instance belongs to one of two classes. Logistic regression models the probability that an instance belongs to a particular class using the logistic function (also known as the sigmoid function). The logistic function outputs values between 0 and 1, which can be interpreted as probabilities.\n",
    "\n",
    "The logistic regression model can be mathematically expressed as:\n",
    "\n",
    "P(y=1)=11+e−(b0+b1x1+b2x2+…+bnxn)\n",
    "\n",
    "where:\n",
    "\n",
    "    - P(y=1) is the probability of the instance belonging to class 1.\n",
    "    - e is the base of the natural logarithm.\n",
    "    - b0,b1,…,bn are the coefficients.\n",
    "    - x1,x2,…,xn​ are the input features.\n",
    "\n",
    "Logistic regression is widely used in various domains, such as healthcare for disease prediction, marketing for customer churn prediction, and finance for credit scoring.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example of using logistic regression for binary classification using the famous Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset (you may need to adjust the path)\n",
    "titanic = pd.read_csv('path_to_titanic_dataset/titanic.csv')\n",
    "\n",
    "# Drop rows with missing values and select relevant features\n",
    "titanic = titanic.dropna(subset=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived'])\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, logistic regression is applied to predict whether passengers on the Titanic survived or not. The model is trained on features such as passenger class, gender, age, number of siblings/spouses aboard, number of parents/children aboard, fare, and embarkation port. The accuracy, classification report, and confusion matrix are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.1. Metrics for evaluating classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of a classifier is crucial to understanding how well it generalizes to new, unseen data. Various metrics provide insights into different aspects of a classifier's performance. Here are some common metrics:\n",
    "\n",
    "1. Accuracy:\n",
    "    Accuracy is the ratio of correctly predicted instances to the total instances. It provides an overall measure of classification correctness.\n",
    "    Accuracy=True Positives+True NegativesTotal InstancesAccuracy=Total InstancesTrue Positives+True Negatives​\n",
    "\n",
    "2. Precision:\n",
    "    Precision is the ratio of correctly predicted positive observations to the total predicted positives. It measures the accuracy of the positive predictions.\n",
    "    Precision=True PositivesTrue Positives+False PositivesPrecision=True Positives+False PositivesTrue Positives​\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "    Recall is the ratio of correctly predicted positive observations to the total actual positives. It measures the ability of the classifier to capture all positive instances.\n",
    "    Recall=True PositivesTrue Positives+False NegativesRecall=True Positives+False NegativesTrue Positives​\n",
    "\n",
    "4. F1 Score:\n",
    "    The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "    F1 Score=2×Precision×RecallPrecision+RecallF1 Score=Precision+Recall2×Precision×Recall​\n",
    "\n",
    "5. Confusion Matrix:\n",
    "    A confusion matrix is a table that summarizes the classifier's performance, showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "6. Receiver Operating Characteristic (ROC) Curve:\n",
    "    The ROC curve plots the true positive rate against the false positive rate at various threshold settings. It helps visualize the trade-off between sensitivity and specificity.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Titanic dataset and evaluate the performance of a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset (you may need to adjust the path)\n",
    "titanic = pd.read_csv('path_to_titanic_dataset/titanic.csv')\n",
    "\n",
    "# Drop rows with missing values and select relevant features\n",
    "titanic = titanic.dropna(subset=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived'])\n",
    "X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, logistic regression is used to predict survival on the Titanic. The classifier's performance is evaluated using accuracy, precision, recall, F1 score, and a confusion matrix. Additionally, an ROC curve is plotted to visualize the trade-off between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.2. Holdout method and random subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Holdout Method and Random Subsampling are techniques used in machine learning for evaluating the performance of a model. They involve splitting the dataset into training and testing sets to assess how well the model generalizes to new, unseen data.\n",
    "\n",
    "#### Holdout Method:\n",
    "    In the Holdout Method, the dataset is divided into two parts: a training set and a testing set. The training set is used to train the model, while the testing set is reserved for evaluating its performance. Common split ratios include 70-30, 80-20, or 90-10, depending on the size of the dataset.\n",
    "\n",
    "#### Random Subsampling (or k-Fold Cross-Validation):\n",
    "    Random Subsampling involves dividing the dataset into k subsets (folds). The model is trained k times, each time using k-1 folds for training and the remaining fold for testing. This process is repeated k times, with each fold serving as the testing set exactly once. The performance metrics are then averaged across all iterations.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset and the Holdout Method for evaluating a k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets using the Holdout Method\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a k-NN classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Iris dataset is split into training and testing sets using the Holdout Method. A k-NN classifier is then trained on the training set and evaluated on the testing set. The accuracy and classification report are displayed to assess the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.3. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling technique used to assess the performance and generalization of a machine learning model. It helps to overcome the limitations of a single train-test split by providing a more robust estimate of the model's performance. The most common form of cross-validation is k-Fold Cross-Validation, where the dataset is divided into k subsets (folds). The model is trained k times, each time using k-1 folds for training and the remaining fold for testing. This process is repeated k times, with each fold serving as the testing set exactly once. The performance metrics are then averaged across all iterations.\n",
    "\n",
    "The steps of k-Fold Cross-Validation are as follows:\n",
    "\n",
    "1. Split Data:\n",
    "    Divide the dataset into k equally sized folds.\n",
    "\n",
    "2. Train-Test Iterations:\n",
    "    For each iteration, use k-1 folds for training and the remaining fold for testing.\n",
    "\n",
    "3. Performance Metrics:\n",
    "    Evaluate the model's performance on each iteration and record the performance metrics.\n",
    "\n",
    "4. Average Metrics:\n",
    "    Calculate the average of the recorded performance metrics across all iterations.\n",
    "\n",
    "#### Benefits of Cross-Validation:\n",
    "\n",
    "    - Provides a more reliable estimate of a model's performance.\n",
    "\n",
    "    - Reduces the impact of variability in a single train-test split.\n",
    "\n",
    "    - Utilizes the entire dataset for training and testing.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset and k-Fold Cross-Validation with a support vector machine (SVM) classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a support vector machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Perform k-Fold Cross-Validation (k=5)\n",
    "cv_scores = cross_val_score(svm_classifier, X, y, cv=5)\n",
    "\n",
    "# Display the cross-validated accuracy scores\n",
    "print(\"Cross-Validated Accuracy Scores:\", cv_scores)\n",
    "print(f\"Average Accuracy: {cv_scores.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Iris dataset is used to perform k-Fold Cross-Validation with a linear SVM classifier. The cross_val_score function from scikit-learn is employed to obtain accuracy scores for each fold. The average accuracy across all folds is then calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.4. Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used to estimate the variability and uncertainty associated with a sample statistic, such as the mean or standard deviation. It involves repeatedly sampling, with replacement, from the observed data to create multiple bootstrap samples. Each bootstrap sample is then used to compute the sample statistic of interest. By analyzing the distribution of these computed statistics across multiple bootstrap samples, one can obtain confidence intervals and make more robust statistical inferences.\n",
    "\n",
    "#### The steps of the Bootstrap method are as follows:\n",
    "\n",
    "1. Original Sample:\n",
    "        Start with the original dataset of size NN.\n",
    "\n",
    "2. Bootstrap Samples:\n",
    "        Generate BB bootstrap samples by randomly selecting NN data points from the original dataset with replacement.\n",
    "\n",
    "3. Statistic Computation:\n",
    "        For each bootstrap sample, compute the sample statistic of interest (e.g., mean, standard deviation).\n",
    "\n",
    "4. Analysis:\n",
    "        Analyze the distribution of computed statistics to estimate variability, confidence intervals, or perform hypothesis testing.\n",
    "\n",
    "#### Benefits of Bootstrap:\n",
    "\n",
    "    - Provides an empirical estimate of the sampling distribution of a statistic.\n",
    "\n",
    "    - Useful when assumptions of parametric statistical methods are violated.\n",
    "\n",
    "    - Robust in the presence of outliers.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to estimate the confidence interval of the mean sepal length using the Bootstrap method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "sepal_length = iris.data[:, 0]\n",
    "\n",
    "# Set the number of bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "# Generate bootstrap samples and compute means\n",
    "for i in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = resample(sepal_length, replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Display the results\n",
    "print(\"Bootstrap Sample Mean: {:.2f}\".format(np.mean(bootstrap_means)))\n",
    "print(\"95% Confidence Interval: [{:.2f}, {:.2f}]\".format(confidence_interval[0], confidence_interval[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Bootstrap method is used to estimate the 95% confidence interval of the mean sepal length in the Iris dataset. The resample function from scikit-learn is utilized to generate bootstrap samples. The mean of each bootstrap sample is computed, and the distribution of bootstrap sample means is used to estimate the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.5. Model selection using statistical tests of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is a critical step in the data mining process, involving the identification and evaluation of different models to choose the one that best fits the data. Statistical tests of significance can be employed for model selection by comparing the performance of different models and determining if the observed differences are statistically significant.\n",
    "\n",
    "#### The process typically involves the following steps:\n",
    "\n",
    "1. Select Candidate Models:\n",
    "    Choose a set of candidate models that are relevant to the problem at hand.\n",
    "\n",
    "2. Train Models:\n",
    "    Train each model on the training dataset.\n",
    "\n",
    "3. Evaluate Models:\n",
    "    Evaluate the performance of each model on a validation dataset or through cross-validation.\n",
    "\n",
    "4. Statistical Testing:\n",
    "    Apply statistical tests to compare the performance metrics of the models.\n",
    "\n",
    "5. Select Best Model:\n",
    "    Choose the model with the best performance, considering both practical significance and statistical significance.\n",
    "\n",
    "Commonly used statistical tests for model selection include t-tests, ANOVA, or their non-parametric counterparts. These tests help determine if observed differences in performance metrics are likely due to genuine differences in model effectiveness rather than random chance.\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to compare the performance of two classification models (e.g., Decision Tree and Random Forest) using a t-test for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create classifiers (Decision Tree and Random Forest)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "dt_scores = cross_val_score(dt_classifier, X, y, cv=5, scoring='accuracy')\n",
    "rf_scores = cross_val_score(rf_classifier, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform a paired t-test for accuracy\n",
    "t_stat, p_value = ttest_rel(dt_scores, rf_scores)\n",
    "\n",
    "# Display the results\n",
    "print(\"Decision Tree Mean Accuracy: {:.2f}\".format(dt_scores.mean()))\n",
    "print(\"Random Forest Mean Accuracy: {:.2f}\".format(rf_scores.mean()))\n",
    "print(\"Paired t-test p-value: {:.4f}\".format(p_value))\n",
    "\n",
    "# Check for statistical significance (e.g., p-value < 0.05)\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in accuracy is statistically significant. Choose the model accordingly.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in accuracy between the models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, cross-validation is performed for both Decision Tree and Random Forest classifiers on the Iris dataset. A paired t-test is then conducted to determine if there is a statistically significant difference in accuracy between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.6. Comparing classifiers based on cost–benefit and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing classifiers involves assessing their performance using various metrics and visualization techniques. Two common approaches are based on cost–benefit analysis and Receiver Operating Characteristic (ROC) curves.\n",
    "\n",
    "1. Cost–Benefit Analysis:\n",
    "    Cost–benefit analysis involves considering the practical consequences of classification decisions. It assigns costs and benefits to different outcomes (true positive, false positive, true negative, false negative) and calculates a total cost or benefit. This approach is especially useful when the consequences of false positives and false negatives are different.\n",
    "\n",
    "2. ROC Curves:\n",
    "    ROC curves graphically depict the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different classification thresholds. The area under the ROC curve (AUC-ROC) summarizes the overall performance of the classifier across various threshold settings.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to compare two classifiers (e.g., Logistic Regression and Random Forest) based on cost–benefit analysis and ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create classifiers (Logistic Regression and Random Forest)\n",
    "logreg_classifier = LogisticRegression()\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifiers\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "logreg_predictions = logreg_classifier.predict(X_test)\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Cost–Benefit Analysis (Example: Assuming cost of false negative is 5 times the cost of false positive)\n",
    "cost_factor = 5\n",
    "cost_benefit_logreg = confusion_matrix(y_test, logreg_predictions) * np.array([[0, 1], [cost_factor, 0]]).T\n",
    "cost_benefit_rf = confusion_matrix(y_test, rf_predictions) * np.array([[0, 1], [cost_factor, 0]]).T\n",
    "\n",
    "# Display the cost–benefit matrices\n",
    "print(\"Cost–Benefit Matrix (Logistic Regression):\\n\", cost_benefit_logreg)\n",
    "print(\"Total Cost (Logistic Regression):\", np.sum(cost_benefit_logreg))\n",
    "\n",
    "print(\"\\nCost–Benefit Matrix (Random Forest):\\n\", cost_benefit_rf)\n",
    "print(\"Total Cost (Random Forest):\", np.sum(cost_benefit_rf))\n",
    "\n",
    "# ROC Curves\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, logreg_classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc_logreg = auc(fpr_logreg, tpr_logreg)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label=f'Logistic Regression (AUC = {roc_auc_logreg:.2f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Logistic Regression and Random Forest classifiers are trained on the Breast Cancer Wisconsin dataset. Cost–benefit analysis is performed based on assumed costs of false positives and false negatives. ROC curves are plotted to compare the classifiers' performance in terms of sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.1. Introducing ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are machine learning techniques that combine predictions from multiple individual models to create a more robust and accurate model. The idea is to leverage the diversity among different models to improve overall predictive performance. Ensemble methods can be applied to both classification and regression tasks.\n",
    "\n",
    "#### There are two main types of ensemble methods:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating):\n",
    "    Bagging involves training multiple instances of the same model on different subsets of the training data, often created through bootstrapping (sampling with replacement). The final prediction is obtained by averaging (for regression) or voting (for classification) over the predictions of individual models. Random Forest is a popular bagging algorithm.\n",
    "\n",
    "2. Boosting:\n",
    "    Boosting builds a sequence of weak learners (models that perform slightly better than random chance) sequentially, with each model focusing on correcting the errors of its predecessor. Boosting assigns weights to training instances, giving more importance to misclassified instances. AdaBoost and Gradient Boosting are common boosting algorithms.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of an ensemble method, specifically the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier is trained on the Iris dataset. The classifier is an ensemble of decision trees, where each tree is trained on a different subset of the data. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.2. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that involves training multiple instances of the same model on different subsets of the training data. The primary idea behind bagging is to introduce diversity among individual models by creating these subsets through bootstrapping, a sampling technique where instances are randomly selected with replacement.\n",
    "\n",
    "#### The key steps in bagging are as follows:\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "    Randomly select subsets of the training data with replacement (bootstrapping) to create multiple training datasets.\n",
    "\n",
    "2. Model Training:\n",
    "    Train a base model (e.g., decision tree, neural network) independently on each bootstrap sample.\n",
    "\n",
    "3. Aggregation:\n",
    "    Combine the predictions of individual models through averaging (for regression) or voting (for classification) to obtain the final ensemble prediction.\n",
    "\n",
    "4. Diversity:\n",
    "    The diversity among models comes from the different subsets of data used for training, which helps improve the model's generalization performance.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to demonstrate the application of bagging with the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier, a bagging algorithm, is trained on the Breast Cancer Wisconsin dataset. The classifier consists of an ensemble of decision trees, each trained on a different subset of the data created through bootstrapping. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that builds a sequence of weak learners (models that perform slightly better than random chance) sequentially. Each model in the sequence focuses on correcting the errors of its predecessor. The key idea behind boosting is to assign weights to training instances, giving more importance to instances that were misclassified by the previous models.\n",
    "\n",
    "#### The primary steps in boosting are as follows:\n",
    "\n",
    "1. Weight Assignment:\n",
    "    Assign equal weights to all training instances initially.\n",
    "\n",
    "2. Model Training:\n",
    "    Train a weak learner (e.g., decision tree, shallow neural network) on the training data with the assigned weights.\n",
    "\n",
    "3. Prediction and Error Calculation:\n",
    "    Make predictions on the training data and calculate the errors.\n",
    "\n",
    "4. Instance Weight Update:\n",
    "    Increase the weights of misclassified instances, making them more influential in the next model.\n",
    "\n",
    "5. Repeat:\n",
    "    Repeat steps 2-4 for a predefined number of iterations or until a performance threshold is reached.\n",
    "\n",
    "6. Final Prediction:\n",
    "    Combine the predictions of all models with each model's weight, often using a weighted sum, to obtain the final ensemble prediction.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of a boosting algorithm, specifically AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoost classifier with 50 weak learners (Decision Trees)\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, an AdaBoost classifier is trained on the Iris dataset. AdaBoost sequentially trains multiple weak learners (Decision Trees by default) and assigns higher weights to instances that are misclassified by previous models. The final prediction is a weighted sum of the individual model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.4. Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning technique that combines the power of bagging with decision trees to create a robust and accurate predictive model. It builds multiple decision trees during training and merges their predictions to obtain a more stable and reliable result. The \"random\" in Random Forest comes from the introduction of randomness at two levels: in the selection of data samples used for training each tree (bootstrap sampling) and in the selection of features considered at each split of the trees.\n",
    "\n",
    "#### The key characteristics of Random Forest are:\n",
    "\n",
    "1. Bootstrap Sampling:\n",
    "    For each tree, a random sample of the training data is selected with replacement. This process is known as bootstrap sampling.\n",
    "\n",
    "2. Random Feature Selection:\n",
    "    At each node of the decision tree, a random subset of features is considered for splitting. This introduces diversity among the trees and helps prevent overfitting.\n",
    "\n",
    "3. Aggregation:\n",
    "    The predictions of individual trees are combined through averaging (for regression) or voting (for classification) to obtain the final ensemble prediction.\n",
    "\n",
    "4. Robustness:\n",
    "    Random Forest is less prone to overfitting and is generally more robust compared to individual decision trees.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset to demonstrate the application of Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Random Forest classifier is trained on the Breast Cancer Wisconsin dataset. The classifier consists of an ensemble of decision trees, each trained on a different subset of the data created through bootstrap sampling. The final prediction is obtained by aggregating the individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.5. Improving classification accuracy of class-imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class-imbalanced datasets are characterized by a significant difference in the number of instances between different classes. In such scenarios, standard machine learning models might be biased towards the majority class, leading to suboptimal performance on the minority class. Improving classification accuracy on imbalanced data involves addressing the imbalance to ensure that the model generalizes well across all classes.\n",
    "Techniques to Improve Classification Accuracy on Imbalanced Data:\n",
    "\n",
    "1. Resampling:\n",
    "    Over-sampling: Increase the number of instances in the minority class by replicating or generating synthetic examples.\n",
    "    Under-sampling: Decrease the number of instances in the majority class by randomly removing examples.\n",
    "\n",
    "2. Weighted Loss Functions:\n",
    "    Assign different weights to classes in the loss function during model training. This gives more importance to the minority class.\n",
    "\n",
    "3. Ensemble Methods:\n",
    "    Utilize ensemble methods like Random Forest or AdaBoost, which can handle class imbalances better than individual models.\n",
    "\n",
    "4. Anomaly Detection:\n",
    "    Treat the minority class as an anomaly and use anomaly detection techniques to identify instances of the minority class.\n",
    "\n",
    "5. Cost-sensitive Learning:\n",
    "    Introduce misclassification costs to make the model penalize errors in the minority class more than in the majority class.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset where we artificially introduce class imbalance. We'll use the Synthetic Minority Over-sampling Technique (SMOTE) for over-sampling the minority class and a Random Forest classifier for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Introduce class imbalance (assuming class 0 is the minority class)\n",
    "y[y == 0] = 1\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE for over-sampling the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the resampled training data\n",
    "random_forest_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we introduce class imbalance in the Iris dataset and apply SMOTE for over-sampling the minority class. Then, we train a Random Forest classifier on the resampled data and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1. Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a crucial step in the data preprocessing phase, involving the identification and selection of the most relevant features for a particular task. Filter methods are a category of feature selection techniques that evaluate the relevance of features based on certain statistical measures or scoring criteria. These methods assess the characteristics of individual features independently of the machine learning model.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "1. Independence:\n",
    "        Filter methods assess each feature's relevance independently of other features.\n",
    "\n",
    "2. Scoring Criteria:\n",
    "        Features are ranked or scored based on statistical measures, such as correlation, mutual information, or statistical tests.\n",
    "\n",
    "3. Preprocessing:\n",
    "        Filter methods are applied as a preprocessing step before training a machine learning model.\n",
    "\n",
    "4. Selection Threshold:\n",
    "        A threshold is set to select the top-ranked features, and the rest are discarded.\n",
    "\n",
    "5. Advantages:\n",
    "        Computationally efficient and can handle high-dimensional data.\n",
    "        Model-agnostic, making them suitable for various algorithms.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use the chi-squared (χ²) statistical test as a filter method to select the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SelectKBest with chi-squared test for feature selection\n",
    "k_best_selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best_selector.transform(X_test)\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the selected features\n",
    "random_forest_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = random_forest_classifier.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the chi-squared test through the SelectKBest method to select the top 10 features from the Breast Cancer Wisconsin dataset. The selected features are then used to train a Random Forest classifier, and the model's performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods are a category of feature selection techniques that evaluate subsets of features by training and testing a machine learning model on different combinations. Unlike filter methods, wrapper methods consider the interaction between features and assess subsets based on the model's performance. These methods typically use a search algorithm to explore the feature space and select the optimal subset.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- Model-Dependent:\n",
    "    Wrapper methods are model-dependent, as they involve training and testing a specific machine learning model on different feature subsets.\n",
    "\n",
    "- Search Strategy:\n",
    "    The search strategy can be exhaustive (evaluating all possible subsets) or heuristic (using algorithms like forward selection or backward elimination).\n",
    "\n",
    "- Performance Evaluation:\n",
    "    Model performance serves as the criterion for selecting feature subsets. Common metrics include accuracy, F1-score, or other relevant performance measures.\n",
    "\n",
    "- Computational Intensity:\n",
    "    Wrapper methods can be computationally intensive, especially when evaluating a large number of feature subsets.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use a simple wrapper method, Recursive Feature Elimination (RFE), with a Support Vector Machine (SVM) classifier to select the optimal subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel=\"linear\", random_state=42)\n",
    "\n",
    "# Apply Recursive Feature Elimination (RFE) for feature selection\n",
    "rfe_selector = RFE(estimator=svm_classifier, n_features_to_select=10)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = rfe_selector.transform(X_test)\n",
    "\n",
    "# Train the classifier on the selected features\n",
    "svm_classifier.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test_selected)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use Recursive Feature Elimination (RFE) with a Support Vector Machine (SVM) classifier to select the top 10 features from the Breast Cancer Wisconsin dataset. The selected features are then used to train the SVM classifier, and the model's performance is evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3. Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine feature selection with the model training process. These methods incorporate feature selection as an integral part of the model training, aiming to identify the most relevant features during the learning process. Embedded methods are model-dependent and often leverage regularization techniques to penalize or eliminate irrelevant features.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- Model-Dependent:\n",
    "    Embedded methods are closely tied to specific machine learning models and utilize their built-in feature selection capabilities.\n",
    "\n",
    "- Regularization:\n",
    "    Regularization terms are introduced during model training to penalize the inclusion of unnecessary features, encouraging the model to focus on the most informative ones.\n",
    "\n",
    "- Joint Optimization:\n",
    "    The selection of features and model parameters is jointly optimized during the training process.\n",
    "\n",
    "- Computational Efficiency:\n",
    "    Embedded methods are generally more computationally efficient than wrapper methods, as they do not require external model evaluations for each feature subset.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Breast Cancer Wisconsin dataset. We'll use the LASSO (Least Absolute Shrinkage and Selection Operator) regularization technique with a linear regression model as an embedded method for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LASSO regression model\n",
    "lasso_model = Lasso(alpha=0.01, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance from the LASSO model\n",
    "feature_importance = lasso_model.coef_\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance, tick_label=cancer.feature_names)\n",
    "plt.title(\"LASSO Feature Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient Magnitude\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use LASSO regularization with a linear regression model to select important features from the Breast Cancer Wisconsin dataset. The coefficients obtained from the trained LASSO model indicate the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1. Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Belief Networks (BBNs):\n",
    "\n",
    "Bayesian Belief Networks, also known as Bayesian Networks or Bayesian Graphical Models, are probabilistic graphical models that represent the probabilistic relationships among a set of variables. These networks are based on Bayesian probability theory and utilize a directed acyclic graph (DAG) to illustrate the conditional dependencies between variables. BBNs consist of two main components: nodes and edges.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Nodes:\n",
    "    Nodes represent variables in the system and can be either observed (evidence) or unobserved (hidden).\n",
    "\n",
    "- Edges:\n",
    "    Edges connect nodes and represent the probabilistic dependencies between variables.\n",
    "\n",
    "- Conditional Probability Tables (CPTs):\n",
    "    Each node has a conditional probability table that specifies the probability distribution of that node given its parents in the graph.\n",
    "\n",
    "- D-separation:\n",
    "    D-separation rules determine the independence relationships between variables in the graph.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Pomegranate library in Python to create a Bayesian Belief Network for a diagnostic scenario. We'll model the relationship between symptoms and possible medical conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pomegranate import (DiscreteDistribution, ConditionalProbabilityTable, State, BayesianNetwork)\n",
    "\n",
    "# Define nodes representing symptoms and conditions\n",
    "fever = DiscreteDistribution({'True': 0.1, 'False': 0.9})\n",
    "cough = DiscreteDistribution({'True': 0.3, 'False': 0.7})\n",
    "headache = DiscreteDistribution({'True': 0.2, 'False': 0.8})\n",
    "flu = ConditionalProbabilityTable(\n",
    "    [['True', 'True', 'True', 0.95],\n",
    "     ['True', 'True', 'False', 0.05],\n",
    "     ['True', 'False', 'True', 0.9],\n",
    "     ['True', 'False', 'False', 0.1],\n",
    "     ['False', 'True', 'True', 0.3],\n",
    "     ['False', 'True', 'False', 0.7],\n",
    "     ['False', 'False', 'True', 0.01],\n",
    "     ['False', 'False', 'False', 0.99]], [fever, cough, headache])\n",
    "\n",
    "# Create states for each variable\n",
    "s1 = State(fever, name=\"fever\")\n",
    "s2 = State(cough, name=\"cough\")\n",
    "s3 = State(headache, name=\"headache\")\n",
    "s4 = State(flu, name=\"flu\")\n",
    "\n",
    "# Create a Bayesian Network and add states\n",
    "network = BayesianNetwork(\"Medical Diagnosis\")\n",
    "network.add_states(s1, s2, s3, s4)\n",
    "\n",
    "# Add edges defining the dependencies\n",
    "network.add_edge(s1, s4)\n",
    "network.add_edge(s2, s4)\n",
    "network.add_edge(s3, s4)\n",
    "\n",
    "# Finalize the network\n",
    "network.bake()\n",
    "\n",
    "# Predict the probability of having the flu given symptoms\n",
    "result = network.predict_proba([['True', 'False', 'True', None]])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Pomegranate library to create a simple Bayesian Belief Network for medical diagnosis. The network models the relationships between symptoms (fever, cough, headache) and a medical condition (flu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2. Training Bayesian belief networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Bayesian Belief Networks involves estimating the parameters of the conditional probability tables (CPTs) based on observed data. In many cases, the structure of the Bayesian Network is assumed or predefined, and the focus is on learning the probabilities associated with the edges in the graph. Learning from data helps to improve the accuracy of the network's predictions.\n",
    "\n",
    "#### Training Steps:\n",
    "\n",
    "- Data Collection:\n",
    "    Gather a dataset containing observations of the variables in the Bayesian Network.\n",
    "\n",
    "- Parameter Estimation:\n",
    "    Use statistical methods to estimate the probabilities in the CPTs based on the observed data.\n",
    "\n",
    "- Model Adjustment:\n",
    "    Refine the Bayesian Network structure or adjust parameters to improve model performance.\n",
    "\n",
    "- Validation:\n",
    "    Evaluate the trained model on new data to ensure generalization.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Pomegranate library in Python to train a Bayesian Belief Network for a diagnostic scenario. We'll use a dataset of symptoms and flu cases to estimate the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for training\n",
    "data = [[True, False, True, True],\n",
    "        [False, True, True, True],\n",
    "        [True, True, False, True],\n",
    "        [False, False, True, False],\n",
    "        [True, True, True, True],\n",
    "        [False, True, False, False],\n",
    "        [True, False, False, False],\n",
    "        [False, False, False, False]]\n",
    "\n",
    "# Define nodes representing symptoms and conditions\n",
    "fever = DiscreteDistribution.from_samples(data[:, 0])\n",
    "cough = DiscreteDistribution.from_samples(data[:, 1])\n",
    "headache = DiscreteDistribution.from_samples(data[:, 2])\n",
    "flu = ConditionalProbabilityTable.from_samples(data[:, [0, 1, 2, 3]], [fever, cough, headache])\n",
    "\n",
    "# Create states for each variable\n",
    "s1 = State(fever, name=\"fever\")\n",
    "s2 = State(cough, name=\"cough\")\n",
    "s3 = State(headache, name=\"headache\")\n",
    "s4 = State(flu, name=\"flu\")\n",
    "\n",
    "# Create a Bayesian Network and add states\n",
    "network = BayesianNetwork(\"Medical Diagnosis\")\n",
    "network.add_states(s1, s2, s3, s4)\n",
    "\n",
    "# Add edges defining the dependencies\n",
    "network.add_edge(s1, s4)\n",
    "network.add_edge(s2, s4)\n",
    "network.add_edge(s3, s4)\n",
    "\n",
    "# Finalize the network\n",
    "network.bake()\n",
    "\n",
    "# Display the original probabilities\n",
    "print(\"Original Probabilities:\")\n",
    "print(network.predict_proba([[True, False, True, None]]))\n",
    "\n",
    "# Train the network with the synthetic data\n",
    "network.fit(data)\n",
    "\n",
    "# Display the trained probabilities\n",
    "print(\"\\nTrained Probabilities:\")\n",
    "print(network.predict_proba([[True, False, True, None]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use synthetic data to train a Bayesian Belief Network for medical diagnosis using the Pomegranate library. The network is initially created with predefined probabilities, and then the fit method is used to update the probabilities based on the synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1. Linear support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machines (SVMs):\n",
    "\n",
    "Linear Support Vector Machines are a class of supervised machine learning models used for classification and regression tasks. SVMs operate by finding the hyperplane that best separates the data points of different classes while maximizing the margin between them. In the case of linear SVMs, the decision boundary is a linear hyperplane.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Hyperplane:\n",
    "        The decision boundary that separates data points of different classes. In a linear SVM, this is a straight line in two dimensions or a plane in higher dimensions.\n",
    "\n",
    "- Margin:\n",
    "        The distance between the hyperplane and the nearest data point of each class. SVM aims to maximize this margin.\n",
    "\n",
    "- Support Vectors:\n",
    "        The data points that lie closest to the hyperplane and influence its position. These points are crucial for determining the optimal decision boundary.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of a Linear Support Vector Machine for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Consider only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Support Vector Machine classifier\n",
    "svm_classifier = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_classifier.support_vectors_[:, 0], svm_classifier.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title(\"Linear SVM Decision Boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Linear Support Vector Machine is applied to the Iris dataset, considering only the first two features for visualization purposes. The model is trained on the training data, and its decision boundary is plotted along with the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2. Nonlinear support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear Support Vector Machines:\n",
    "\n",
    "While linear Support Vector Machines (SVMs) are effective for linearly separable data, nonlinear SVMs extend the model's capability to handle complex relationships in the data by employing kernel functions. Kernel functions transform the original feature space into a higher-dimensional space, making it possible to find nonlinear decision boundaries.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Kernel Functions:\n",
    "    Mathematical functions that compute the dot product between data points in a higher-dimensional space without explicitly calculating the transformation.\n",
    "\n",
    "- Radial Basis Function (RBF) Kernel:\n",
    "    Commonly used kernel for nonlinear SVMs, allowing the model to capture complex patterns in the data.\n",
    "\n",
    "- Gamma Parameter:\n",
    "    A parameter in the RBF kernel that influences the shape of the decision boundary. Higher gamma values result in a more complex boundary.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to demonstrate the application of a Nonlinear Support Vector Machine with an RBF kernel for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Consider only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Nonlinear Support Vector Machine classifier with RBF kernel\n",
    "svm_classifier = SVC(kernel=\"rbf\", C=1.0, gamma=0.1, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "                     np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_classifier.support_vectors_[:, 0], svm_classifier.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title(\"Nonlinear SVM Decision Boundary (RBF Kernel)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, a Nonlinear Support Vector Machine with an RBF kernel is applied to the Iris dataset, considering only the first two features for visualization. The model is trained on the training data, and its decision boundary is plotted along with the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1. Using IF-THEN rules for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IF-THEN Rules for Classification:\n",
    "\n",
    "Rule-based classification involves the creation of a set of rules that determine the class or category of an instance based on its feature values. Each rule typically takes the form \"IF condition THEN class.\" These rules are human-readable and provide transparency into the decision-making process. Rule-based systems are often employed in scenarios where interpretability and explainability are crucial.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- IF-THEN Structure:\n",
    "    Each rule specifies a condition based on feature values, and the corresponding action (classification) to take if the condition is met.\n",
    "\n",
    "- Rule Order:\n",
    "    Rules are usually evaluated sequentially, and the first rule that matches the conditions is applied.\n",
    "\n",
    "- Interpretability:\n",
    "    Rule-based systems are transparent, making them easy to interpret and explain.\n",
    "\n",
    "- Rule Learning:\n",
    "    Rule-based models can be manually crafted or learned from data through techniques like decision tree induction.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the famous Iris dataset to demonstrate the application of rule-based classification. We'll use the \"fuzzy\" library in Python to define IF-THEN rules based on fuzzy logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzy import FuzzySystem, Rule, Antecedent, Consequent\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define fuzzy antecedents\n",
    "sepal_length = Antecedent(\"Sepal Length\", X_train[:, 0])\n",
    "sepal_width = Antecedent(\"Sepal Width\", X_train[:, 1])\n",
    "petal_length = Antecedent(\"Petal Length\", X_train[:, 2])\n",
    "petal_width = Antecedent(\"Petal Width\", X_train[:, 3])\n",
    "\n",
    "# Define fuzzy consequents\n",
    "setosa = Consequent(\"Setosa\", y_train == 0)\n",
    "versicolor = Consequent(\"Versicolor\", y_train == 1)\n",
    "virginica = Consequent(\"Virginica\", y_train == 2)\n",
    "\n",
    "# Define IF-THEN rules based on fuzzy logic\n",
    "rules = [\n",
    "    Rule(sepal_length[\"low\"] | sepal_width[\"medium\"], setosa),\n",
    "    Rule(petal_length[\"medium\"] & petal_width[\"high\"], versicolor),\n",
    "    Rule(sepal_length[\"high\"] & petal_length[\"medium\"], virginica),\n",
    "]\n",
    "\n",
    "# Create the fuzzy system\n",
    "fuzzy_system = FuzzySystem(rules)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = fuzzy_system.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"fuzzy\" library to define IF-THEN rules based on fuzzy logic for classifying Iris flowers. Fuzzy logic allows us to express rules in a more flexible manner than traditional crisp logic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2. Rule extraction from a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule extraction involves transforming the decision rules embedded in a decision tree model into a set of explicit IF-THEN rules. Decision trees inherently provide a set of rules used for classification, but extracting these rules can enhance interpretability and facilitate the manual creation or modification of rules.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Decision Tree Rules:\n",
    "    Decision trees make decisions based on a set of rules inferred from the features of the data.\n",
    "\n",
    "- Leaf Nodes:\n",
    "    Each leaf node in a decision tree corresponds to a specific class or outcome.\n",
    "\n",
    "- Rule Extraction Process:\n",
    "    The process involves traversing the decision tree and extracting the conditions present on the path from the root to each leaf.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the Iris dataset to demonstrate the extraction of rules from a decision tree. We'll use the \"sklearn.tree\" module in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Extract rules from the decision tree\n",
    "tree_rules = export_text(decision_tree_classifier, feature_names=iris.feature_names)\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"Extracted Decision Tree Rules:\")\n",
    "print(tree_rules)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset to train a decision tree classifier and extract rules from the trained model using the export_text function from the \"sklearn.tree\" module. The extracted rules provide a clear and human-readable representation of the decision tree's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3. Rule induction using a sequential covering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule Induction Using a Sequential Covering Algorithm:\n",
    "\n",
    "Rule induction involves the automatic generation of rules from a dataset without relying on a predefined model structure. Sequential covering algorithms are a class of rule induction techniques that iteratively discover rules by selecting instances and covering them with rules. This process continues until all instances are covered or a stopping criterion is met.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Sequential Covering:\n",
    "        The algorithm iteratively selects instances not covered by existing rules and generates rules specifically for those instances.\n",
    "\n",
    "- Rule Quality Measures:\n",
    "        The algorithm typically uses quality measures to assess the usefulness of a rule, such as support, confidence, or information gain.\n",
    "\n",
    "- Iterative Process:\n",
    "        The process continues until a predefined stopping criterion is satisfied, such as covering all instances or reaching a certain rule complexity.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"RuleFit\" library in Python to perform rule induction using a sequential covering algorithm. RuleFit is a hybrid model that combines decision trees with linear models to create interpretable rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rulefit import RuleFit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RuleFit classifier\n",
    "rulefit_classifier = RuleFit()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rulefit_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rulefit_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"\\nExtracted Rules:\")\n",
    "for rule in rulefit_classifier.get_rules():\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"RuleFit\" library to perform rule induction using a sequential covering algorithm on the Iris dataset. The RuleFit class is used to train a model that combines decision trees and linear models, providing interpretable rules. The extracted rules can be displayed and analyzed for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4. Associative classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associative classification, also known as rule-based classification or classification by association, combines the principles of association rule mining with traditional classification. Instead of generating rules separately, associative classification simultaneously discovers association rules and utilizes them for classification purposes. This approach often leverages techniques like Apriori algorithm for mining frequent itemsets and a classifier for generating rules based on these itemsets.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "-  Association Rule Mining:\n",
    "    Identifying frequent patterns or associations among variables in the dataset.\n",
    "\n",
    "- Rule Generation:\n",
    "    Deriving classification rules from frequent itemsets discovered during association rule mining.\n",
    "\n",
    "- Hybrid Approach:\n",
    "    Integrating the strengths of both association rule mining and classification to enhance the predictive performance of the model.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"pyARC\" library in Python for associative classification. The pyARC library provides functionalities for mining and using classification rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyARC library if not installed\n",
    "# !pip install pyarc\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyarc import CBA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CBA (Classification Based on Associations) classifier\n",
    "cba_classifier = CBA(support=0.2, confidence=0.7)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "cba_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cba_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "# Display the extracted rules\n",
    "print(\"\\nExtracted Rules:\")\n",
    "for rule in cba_classifier.clf.rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"pyARC\" library to perform associative classification on the Iris dataset. The CBA class is used to create a classifier based on associations, and it is trained on the dataset. The extracted rules can be displayed and analyzed for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5. Discriminative frequent pattern–based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative frequent pattern–based classification is an approach that focuses on finding frequent patterns that are highly correlated with specific classes in the dataset. Instead of generating rules based on associations in the entire dataset, this method aims to discover patterns that discriminate between different classes effectively.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Frequent Pattern Mining:\n",
    "        Identifying patterns that occur frequently in the dataset.\n",
    "\n",
    "- Discriminative Patterns:\n",
    "        Patterns that exhibit significant differences in occurrence between different classes.\n",
    "\n",
    "- Classification based on Discriminative Patterns:\n",
    "        Using the discovered discriminative patterns to build a classifier that can effectively differentiate between classes.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"pyFIM\" library in Python for discriminative frequent pattern–based classification. The pyFIM library provides functionalities for frequent itemset mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyfim library if not installed\n",
    "# !pip install pyfim\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyfim import eclat\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the dataset into a transaction format (list of sets)\n",
    "transactions = [set(map(str, x)) for x in X_train]\n",
    "\n",
    "# Perform discriminative frequent pattern mining using Eclat algorithm\n",
    "patterns = eclat(transactions, target=\"c\", supp=0.2, zmin=2)\n",
    "\n",
    "# Display the discovered discriminative patterns\n",
    "print(\"Discovered Discriminative Patterns:\")\n",
    "for pattern in patterns:\n",
    "    print(pattern)\n",
    "\n",
    "# Create a simple classifier based on the discovered patterns\n",
    "def classify(transaction):\n",
    "    for pattern in patterns:\n",
    "        if pattern.issubset(transaction):\n",
    "            return pattern[-1]  # Class label associated with the pattern\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = [classify(set(map(str, x))) for x in X_test]\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"pyFIM\" library to perform discriminative frequent pattern–based classification on the Iris dataset. The eclat function is used to mine discriminative frequent patterns, and a simple classifier is created based on the discovered patterns. The accuracy and classification report are then displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1. Semisupervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised classification is a type of weakly supervised learning where the training dataset contains both labeled and unlabeled instances. Traditional supervised learning relies on labeled data for training, while unsupervised learning deals with unlabeled data. Semi-supervised learning aims to leverage the benefits of both by using a small amount of labeled data along with a larger amount of unlabeled data to build a more robust model.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- Labeled and Unlabeled Data:\n",
    "        The training dataset includes instances with known labels (labeled) and instances without labels (unlabeled).\n",
    "\n",
    "- Leveraging Unlabeled Data:\n",
    "        Unlabeled data is used to improve the generalization and performance of the classifier.\n",
    "\n",
    "- Common Techniques:\n",
    "        Self-training, co-training, and multi-view learning are common semi-supervised learning techniques.\n",
    "\n",
    "#### Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"scikit-learn\" library in Python for semi-supervised classification. We'll use a simple dataset and a semi-supervised algorithm known as Label Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Introduce semi-supervision by randomly selecting a portion of labels to be -1 (unlabeled)\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "y_semi_supervised = y.copy()\n",
    "y_semi_supervised[rng.rand(len(y)) < 0.5] = -1  # Label -1 indicates unlabeled\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_semi_supervised, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Label Propagation classifier\n",
    "label_propagation_classifier = LabelPropagation(kernel=\"knn\", n_neighbors=10)\n",
    "\n",
    "# Train the classifier on the training data (including unlabeled instances)\n",
    "label_propagation_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = label_propagation_classifier.predict(X_test)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test[y_test != -1], predictions[y_test != -1])\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test[y_test != -1], predictions[y_test != -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset and introduce semi-supervision by randomly assigning a portion of labels to be -1 (unlabeled). The Label Propagation algorithm is then used to perform semi-supervised classification. The accuracy and classification report are displayed, showcasing the potential of leveraging both labeled and unlabeled data for improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2. Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active learning is a semi-supervised learning approach where the algorithm interacts with an \"oracle\" or a human annotator to intelligently query for labels on instances it finds most informative. Instead of passively receiving labeled instances, the algorithm actively selects which instances to query for labels, aiming to maximize learning efficiency with a minimal number of labeled examples.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "    Query Strategy:\n",
    "        The algorithm employs a query strategy to select instances that are expected to provide the most information about the underlying model.\n",
    "\n",
    "    Model Uncertainty:\n",
    "        Instances with uncertain predictions or those near the decision boundary are often prioritized for labeling.\n",
    "\n",
    "    Reducing Annotation Costs:\n",
    "        Active learning aims to reduce the need for large labeled datasets by focusing on informative instances, making it particularly useful when obtaining labeled data is expensive or time-consuming.\n",
    "\n",
    "Practical Example in Python:\n",
    "\n",
    "Let's consider a practical example using the \"modAL\" library in Python for active learning. We'll use a simple synthetic dataset and a basic classifier to demonstrate the active learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and pool sets\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# Create a random forest classifier (base learner)\n",
    "learner = ActiveLearner(estimator=RandomForestClassifier(), X_training=X_train, y_training=y_train)\n",
    "\n",
    "# Define the query strategy (uncertainty sampling)\n",
    "def query_strategy(classifier, X_pool):\n",
    "    uncertainty = classifier.predict_proba(X_pool)[:, 0]  # Example: uncertainty as probability of class 0\n",
    "    return uncertainty.argsort()[-1:]  # Query the instance with the highest uncertainty\n",
    "\n",
    "# Active learning loop\n",
    "n_queries = 50\n",
    "for _ in range(n_queries):\n",
    "    query_idx, query_instance = learner.query(X_pool, n_instances=1, query_strategy=query_strategy)\n",
    "    learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "    X_pool, y_pool = np.delete(X_pool, query_idx, axis=0), np.delete(y_pool, query_idx)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "predictions = learner.predict(X)\n",
    "\n",
    "# Display the accuracy and classification report\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the \"modAL\" library to demonstrate active learning with a random forest classifier. The query strategy is based on uncertainty sampling, where the algorithm selects instances with the highest uncertainty for labeling. The active learning loop iteratively queries the oracle, updates the model, and repeats the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.3. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique where knowledge gained from training a model on one task is applied to improve performance on a different but related task. This is especially useful when labeled data is scarce for the target task. In the context of weak supervision, transfer learning can help leverage information from a source domain with abundant labeled data to boost the performance of a model in a target domain with limited labeled data.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example using transfer learning for text classification. We'll use the transformers library in Python, which provides pre-trained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Example data (replace with your dataset)\n",
    "source_texts = [\"Positive review 1\", \"Positive review 2\", \"Negative review 1\", \"Negative review 2\"]\n",
    "target_texts = [\"New positive review\", \"New negative review\"]\n",
    "\n",
    "# Tokenize and prepare input tensors for source and target domains\n",
    "source_inputs = tokenizer(source_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "target_inputs = tokenizer(target_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Fine-tune the model on the source domain\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "source_labels = torch.tensor([1, 1, 0, 0])  # Binary labels for the source domain\n",
    "source_outputs = model(**source_inputs, labels=source_labels)\n",
    "loss = source_outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Use the fine-tuned model for the target domain\n",
    "target_outputs = model(**target_inputs)\n",
    "target_predictions = torch.argmax(target_outputs.logits, dim=1)\n",
    "\n",
    "print(\"Predictions for target domain:\", target_predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a pre-trained BERT model on a source domain with labeled data (positive and negative reviews). We then fine-tune the model on this source domain. Finally, we apply the fine-tuned model to make predictions on a target domain with limited labeled data (new reviews). Transfer learning helps the model leverage knowledge from the source domain to improve classification performance on the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.4. Distant supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distant supervision is a technique that leverages auxiliary, potentially noisy, or imperfect sources of supervision to train models. This approach is particularly useful when direct labeling of instances is challenging, but there exist distant or indirect sources of information related to the task. By associating labels from these distant sources with instances in the dataset, models can learn effectively despite limited direct supervision.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example using distant supervision for sentiment analysis. We'll use a dataset of tweets that are labeled with sentiment, and we'll also leverage emoticons as a distant supervision signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "data = {'text': [\"I love this product 😍\", \"Not happy with the service 😞\", \"Amazing experience! 😊\", \"Disappointed 😔\"],\n",
    "        'sentiment': ['positive', 'negative', 'positive', 'negative']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Distant supervision: Use emoticons as additional labels\n",
    "df['emoticon_label'] = df['text'].apply(lambda x: 'positive' if '😍' in x else 'negative')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature engineering: Convert text to features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a model using distant supervision labels\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, df.loc[X_train.index, 'emoticon_label'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use emoticons as a distant supervision signal to improve sentiment analysis. The model is trained on the labeled dataset, and additional labels from emoticons are incorporated during training. The resulting model is then evaluated on a test set, showcasing the integration of distant supervision in a practical context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.5. Zero-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot learning is an innovative approach that enables models to generalize to classes or tasks for which they have never seen explicit examples during training. Instead of relying on labeled instances for all classes, zero-shot learning leverages auxiliary information or attributes to make predictions in unseen scenarios. This is particularly beneficial in situations where obtaining labeled data for every possible class is impractical.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of zero-shot learning using a pre-trained language model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Example text data (replace with your dataset)\n",
    "texts = [\n",
    "    \"A delicious recipe for homemade pizza.\",\n",
    "    \"The latest advancements in artificial intelligence.\",\n",
    "    \"Exploring the wonders of outer space.\",\n",
    "]\n",
    "\n",
    "# Zero-shot classification using a pre-trained language model\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Example class names or labels\n",
    "class_names = [\"cooking\", \"technology\", \"science\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "results = classifier(texts, class_names)\n",
    "\n",
    "# Display the results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted Class:\", results[i]['labels'][0])\n",
    "    print(\"Confidence Score:\", results[i]['scores'][0])\n",
    "    print(\"----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the transformers library to access a zero-shot classification pipeline. The model is not explicitly trained on labeled examples for the specified classes (cooking, technology, science). Instead, it leverages its understanding of language and context to make predictions on these unseen classes. This showcases the power of zero-shot learning in scenarios where traditional supervised training is impractical due to the absence of labeled data for all possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. Stream data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream data classification involves the real-time analysis and classification of data as it is generated. This is common in applications where data arrives continuously and decisions need to be made instantaneously. Techniques for stream data classification often require adaptive models that can evolve over time as new data arrives.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of stream data classification using the scikit-multiflow library in Python. We'll use a synthetic dataset for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.data import SEAGenerator\n",
    "from skmultiflow.lazy import KNNClassifier\n",
    "from skmultiflow.evaluation import EvaluatePrequential\n",
    "\n",
    "# Create a stream data generator\n",
    "stream = SEAGenerator(random_state=42)\n",
    "\n",
    "# Define the classifier (K-Nearest Neighbors)\n",
    "classifier = KNNClassifier(n_neighbors=3)\n",
    "\n",
    "# Evaluate the classifier on the stream data\n",
    "evaluator = EvaluatePrequential(show_plot=True, pretrain_size=1000, max_samples=5000)\n",
    "evaluator.evaluate(stream=stream, model=classifier, model_names=['KNN'])\n",
    "\n",
    "# Note: The pretrain_size and max_samples are set for illustration purposes and can be adjusted based on your specific scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the scikit-multiflow library to simulate a stream data scenario with the SEA dataset. We employ a K-Nearest Neighbors (KNN) classifier, which is suitable for online learning scenarios. The EvaluatePrequential class helps evaluate the classifier's performance over time, making it suitable for stream data classification tasks. Adjust the parameters based on your specific use case and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence classification involves the categorization of data instances that are presented in a sequential manner. This is common in various domains such as natural language processing, time-series analysis, and bioinformatics. Techniques for sequence classification often involve models that can capture dependencies and patterns over time.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of sequence classification using a recurrent neural network (RNN) for sentiment analysis on a dataset of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "data = {'text': [\"I loved the movie!\", \"It was a terrible experience.\", \"Amazing plot twists.\", \"Boring and predictable.\"],\n",
    "        'sentiment': ['positive', 'negative', 'positive', 'negative']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Build an RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=X_train.shape[1]),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use an LSTM-based recurrent neural network for sequence classification. The model is trained on a dataset of movie reviews with corresponding sentiments. The sequences of words in each review are tokenized and padded for input to the model. The LSTM layer enables the model to capture sequential dependencies in the data, making it suitable for sequence classification tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.3. Graph data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph data classification involves predicting labels or categories associated with nodes or entire graphs. This is prevalent in various domains, including social network analysis, bioinformatics, and recommendation systems. Techniques for graph data classification leverage the inherent structure and connectivity in graphs to make predictions.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of graph data classification using the stellargraph library in Python. We'll use a dataset for node classification on a citation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "# Assuming you have an edge list file 'edges.csv' with columns 'source' and 'target', and a node features file 'features.csv'\n",
    "edges = pd.read_csv('edges.csv')\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# Create a StellarGraph from the edge list and node features\n",
    "G = StellarGraph(edges=edges, node_features=features)\n",
    "\n",
    "# Extract node labels for node classification\n",
    "node_labels = G.nodes().astype(int) % 2  # Binary node labels for illustration purposes\n",
    "\n",
    "# Split the dataset\n",
    "train_nodes, test_nodes, y_train, y_test = train_test_split(node_labels.index, node_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform biased random walks to generate node sequences for training\n",
    "rw = BiasedRandomWalk(G)\n",
    "walks = rw.run(nodes=list(train_nodes), length=80, n=10, p=0.5, q=2.0)\n",
    "\n",
    "# Use Skip-gram model to learn node embeddings from the walks\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(walks, vector_size=128, window=5, min_count=0, sg=1, workers=2, epochs=1)\n",
    "\n",
    "# Transform node embeddings into a Pandas DataFrame\n",
    "node_embeddings = pd.DataFrame({node: model.wv[node] for node in model.wv.index_to_key})\n",
    "\n",
    "# Train a classifier on the node embeddings\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(node_embeddings.loc[train_nodes], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(node_embeddings.loc[test_nodes])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the stellargraph library to create a graph from an edge list and node features. We perform biased random walks on the graph and use a Skip-gram model to learn node embeddings. These embeddings are then used to train a classifier, in this case, a Random Forest classifier, for node classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1. Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification extends the binary classification scenario to handle more than two classes. In this setup, the goal is to assign each instance to one of multiple predefined classes. Several algorithms and strategies exist to address multiclass classification challenges.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of multiclass classification using the popular Iris dataset with the Support Vector Machine (SVM) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Iris dataset, a classic multiclass classification problem with three classes (setosa, versicolor, and virginica). We employ a Support Vector Machine (SVM) classifier to learn the patterns in the data and predict the class labels. The accuracy and classification report provide insights into the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2. Distance metric learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance metric learning aims to optimize the metric used to measure the similarity or dissimilarity between data points. By learning a suitable distance metric, it becomes possible to improve the effectiveness of algorithms that rely on distances, such as clustering or nearest neighbors.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of distance metric learning using the metric_learn library in Python. We'll use the well-known Iris dataset and the Large Margin Nearest Neighbor (LMNN) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from metric_learn import LMNN\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Large Margin Nearest Neighbor (LMNN) for distance metric learning\n",
    "lmnn = LMNN(k=3, learn_rate=1e-6)\n",
    "lmnn.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data using the learned metric\n",
    "X_train_transformed = lmnn.transform(X_train)\n",
    "X_test_transformed = lmnn.transform(X_test)\n",
    "\n",
    "# Train a k-Nearest Neighbors classifier on the transformed data\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = knn_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the metric_learn library to apply the Large Margin Nearest Neighbor (LMNN) algorithm for distance metric learning on the Iris dataset. The learned metric is then used to transform the data, and a k-Nearest Neighbors classifier is trained on the transformed data. The accuracy metric is used to assess the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3. Interpretability of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability refers to the ease with which humans can understand and trust the decisions made by a machine learning model. In classification, interpretable models and visualization techniques help in gaining insights into feature importance and decision-making processes.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of interpreting a classification model's decisions using the shap library in Python. We'll use a popular dataset, the Titanic dataset, and a simple model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_data = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "\n",
    "# Preprocess the data (replace with your preprocessing steps)\n",
    "titanic_data = titanic_data.dropna(subset=['Age'])\n",
    "titanic_data = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = titanic_data.drop('Survived', axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Use SHAP (SHapley Additive exPlanations) for interpretability\n",
    "explainer = shap.TreeExplainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the feature importance using a summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the shap library to interpret the decisions of a Random Forest classifier on the Titanic dataset. The shap library provides Shapley values, which can be used to explain the impact of each feature on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.4. Genetic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genetic algorithms are optimization algorithms inspired by the process of natural selection. They iteratively evolve a population of candidate solutions by applying genetic operators such as selection, crossover, and mutation. In the context of feature selection, genetic algorithms can be used to search for an optimal subset of features that maximizes or minimizes a given objective function.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of feature selection using a genetic algorithm with the genetic library in Python. We'll use the popular Iris dataset and a simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from genetic import evolve\n",
    "\n",
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for feature selection\n",
    "def objective_function(features):\n",
    "    # Train a Random Forest classifier with the selected features\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train[:, features], y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = clf.predict(X_test[:, features])\n",
    "    \n",
    "    # Evaluate the model and return the accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Define the search space (features to be selected)\n",
    "search_space = list(range(X.shape[1]))\n",
    "\n",
    "# Use a genetic algorithm to find the optimal feature subset\n",
    "best_features = evolve(\n",
    "    objective_function,\n",
    "    search_space,\n",
    "    population_size=10,\n",
    "    generations=5,\n",
    "    crossover_probability=0.8,\n",
    "    mutation_probability=0.2\n",
    ")\n",
    "\n",
    "print(\"Best feature indices:\", best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the genetic library to perform feature selection with a genetic algorithm on the Iris dataset. The objective_function represents the accuracy of a Random Forest classifier trained with a specific subset of features. The genetic algorithm evolves populations of feature subsets over generations, aiming to find the subset that maximizes the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.5. Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning involves training an agent to make sequential decisions in an environment to maximize a cumulative reward signal. In the context of feature selection, reinforcement learning can be employed to dynamically decide which features to include or exclude during the learning process.\n",
    "Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of feature selection using a reinforcement learning approach with the Stable-Baselines3 library in Python. We'll use a simple classification task with the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.envs import DummyVecEnv\n",
    "\n",
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple environment for feature selection\n",
    "class FeatureSelectionEnv:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.action_space = len(X_train[0])\n",
    "        self.observation_space = len(X_train[0])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.selected_features = np.zeros(len(self.X_train[0]))\n",
    "        self.current_step = 0\n",
    "        return self.selected_features\n",
    "\n",
    "    def step(self, action):\n",
    "        self.selected_features[action] = 1\n",
    "        self.current_step += 1\n",
    "        done = self.current_step == len(self.X_train[0])\n",
    "        accuracy = self.evaluate_model()\n",
    "        reward = accuracy if done else 0\n",
    "        return self.selected_features, reward, done, {}\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        selected_indices = np.where(self.selected_features == 1)[0]\n",
    "        if len(selected_indices) == 0:\n",
    "            return 0\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(self.X_train[:, selected_indices], self.y_train)\n",
    "        predictions = clf.predict(X_test[:, selected_indices])\n",
    "        return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Create the environment\n",
    "env = DummyVecEnv([lambda: FeatureSelectionEnv(X_train, y_train)])\n",
    "\n",
    "# Train a Proximal Policy Optimization (PPO) agent for feature selection\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Extract the selected features from the trained agent\n",
    "selected_features = np.array(model.policy.action_proba(observation=np.ones(len(X_train[0])), actions=None)[0]) > 0.5\n",
    "\n",
    "print(\"Selected features indices:\", np.where(selected_features == 1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Stable-Baselines3 library to train a Proximal Policy Optimization (PPO) agent for feature selection on the Iris dataset. The environment is a simple feature selection environment where the agent can choose which features to include or exclude. The agent is trained to maximize the accuracy of a Random Forest classifier on the selected features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-detection-association-rule-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
