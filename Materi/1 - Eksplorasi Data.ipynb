{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer to E-Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Nominal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal Attributes:\n",
    "\n",
    "Nominal attributes are categorical variables that represent distinct categories or labels with no inherent order or ranking among them. These attributes classify data into various groups based on qualitative differences. Examples include colors, gender, types of animals, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Nominal Attributes:\n",
    "\n",
    "Let's create a simple Python code snippet using the pandas library to work with a dataset containing nominal attributes. We'll load a sample dataset, explore nominal attributes, and perform one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with nominal attributes\n",
    "data = {\n",
    "    'Animal': ['Dog', 'Cat', 'Fish', 'Bird', 'Snake'],\n",
    "    'Color': ['Brown', 'Black', 'Gold', 'Blue', 'Green'],\n",
    "    'Habitat': ['Forest', 'Home', 'Aquarium', 'Sky', 'Jungle']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the nominal attributes\n",
    "nominal_attributes = ['Animal', 'Color', 'Habitat']\n",
    "\n",
    "# Display the unique values in each nominal attribute\n",
    "for attribute in nominal_attributes:\n",
    "    print(f\"Unique values in {attribute}: {df[attribute].unique()}\")\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=nominal_attributes)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three nominal attributes: 'Animal', 'Color', and 'Habitat'.\n",
    "    We explore the unique values in each nominal attribute.\n",
    "    We use pd.get_dummies() to perform one-hot encoding, converting each nominal attribute into binary columns.\n",
    "    The resulting DataFrame (df_encoded) is displayed after one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Attributes:\n",
    "\n",
    "Binary attributes are categorical variables that can take on one of two possible values, typically representing the presence or absence of a certain characteristic. These attributes are fundamental in many datasets, and examples include Yes/No, True/False, 1/0, or any other two distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Binary Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing binary attributes. We'll load a sample dataset, explore binary attributes, and perform basic operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with binary attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'PassedExam': [1, 0, 1, 1, 0],\n",
    "    'EnrolledInCourse': [1, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the binary attributes\n",
    "binary_attributes = ['PassedExam', 'EnrolledInCourse']\n",
    "\n",
    "# Display the count of each unique value in binary attributes\n",
    "for attribute in binary_attributes:\n",
    "    print(f\"Counts for {attribute}:\\n{df[attribute].value_counts()}\\n\")\n",
    "\n",
    "# Perform basic operations on binary attributes\n",
    "df['TotalAttributes'] = df['PassedExam'] + df['EnrolledInCourse']\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"Dataset after performing an operation on binary attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with two binary attributes: 'PassedExam' and 'EnrolledInCourse'.\n",
    "    We explore the counts of each unique value in binary attributes using value_counts().\n",
    "    We perform a basic operation (addition) on binary attributes to create a new attribute, 'TotalAttributes'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Ordinal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Attributes:\n",
    "\n",
    "Ordinal attributes are categorical variables with a meaningful order or ranking among the categories. Unlike nominal attributes, ordinal attributes have a clear, meaningful sequence, but the intervals between them are not necessarily uniform or well-defined. Examples of ordinal attributes include education levels (e.g., elementary, high school, college), customer satisfaction ratings, or socioeconomic classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Ordinal Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing ordinal attributes. We'll load a sample dataset, explore ordinal attributes, and demonstrate how to encode them to preserve the ordinal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with ordinal attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'EducationLevel': ['High School', 'College', 'Elementary', 'College', 'High School'],\n",
    "    'SatisfactionRating': [3, 5, 2, 4, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the ordinal attributes\n",
    "ordinal_attributes = ['EducationLevel', 'SatisfactionRating']\n",
    "\n",
    "# Display the unique values in each ordinal attribute\n",
    "for attribute in ordinal_attributes:\n",
    "    print(f\"Unique values in {attribute}: {df[attribute].unique()}\")\n",
    "\n",
    "# Encode ordinal attributes with meaningful numerical values\n",
    "education_level_mapping = {'Elementary': 1, 'High School': 2, 'College': 3}\n",
    "df['EducationLevelEncoded'] = df['EducationLevel'].map(education_level_mapping)\n",
    "\n",
    "# Display the dataset after encoding ordinal attributes\n",
    "print(\"\\nDataset after encoding ordinal attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with two ordinal attributes: 'EducationLevel' and 'SatisfactionRating'.\n",
    "    We explore the unique values in each ordinal attribute.\n",
    "    We encode ordinal attributes with meaningful numerical values using the map() function to create a new attribute, 'EducationLevelEncoded'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Numeric attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Attributes:\n",
    "\n",
    "Numeric attributes represent quantities and can take on numerical values. There are two main types of numeric attributes: discrete and continuous. Discrete numeric attributes can only take on distinct, separate values (e.g., the number of bedrooms in a house), while continuous numeric attributes can take on any value within a range (e.g., height, weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Handling Numeric Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing numeric attributes. We'll load a sample dataset, explore numeric attributes, and perform basic operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with numeric attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'Age': [21, 19, 22, 20, 23],\n",
    "    'Height (cm)': [175, 160, 180, 165, 185],\n",
    "    'Score': [85, 92, 78, 89, 95]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the numeric attributes\n",
    "numeric_attributes = ['Age', 'Height (cm)', 'Score']\n",
    "\n",
    "# Display summary statistics for numeric attributes\n",
    "print(\"Summary Statistics for Numeric Attributes:\")\n",
    "print(df[numeric_attributes].describe())\n",
    "\n",
    "# Perform basic operations on numeric attributes\n",
    "df['NormalizedScore'] = (df['Score'] - df['Score'].mean()) / df['Score'].std()\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"\\nDataset after performing an operation on numeric attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three numeric attributes: 'Age', 'Height (cm)', and 'Score'.\n",
    "    We explore summary statistics for numeric attributes using describe().\n",
    "    We perform a basic operation (normalization) on the 'Score' attribute to create a new attribute, 'NormalizedScore'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5. Discrete vs. continuous attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Attributes:\n",
    "\n",
    "Definition: Discrete attributes can only take on distinct, separate values.\n",
    "Examples: The number of bedrooms in a house, the count of items in a shopping cart, the number of students in a class.\n",
    "Nature: These attributes are often counted in whole numbers and have clear boundaries between values.\n",
    "\n",
    "#### Continuous Attributes:\n",
    "\n",
    "Definition: Continuous attributes can take on any value within a range.\n",
    "Examples: Height, weight, temperature, and any measurement that can have decimal values.\n",
    "Nature: These attributes have a continuous and infinite set of possible values, making them suitable for measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Discrete vs. Continuous Attributes:\n",
    "\n",
    "Let's create a Python code snippet using the pandas library to work with a dataset containing both discrete and continuous attributes. We'll load a sample dataset, explore the nature of each type, and perform basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset with discrete and continuous attributes\n",
    "data = {\n",
    "    'StudentID': [1, 2, 3, 4, 5],\n",
    "    'NumCourses': [4, 5, 3, 6, 4],  # Discrete attribute (number of courses)\n",
    "    'GPA': [3.5, 4.0, 3.2, 3.8, 3.9],  # Continuous attribute (GPA)\n",
    "    'Income': [25000, 30000, 20000, 35000, 32000]  # Continuous attribute (income in dollars)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Explore the nature of attributes\n",
    "print(\"Nature of Attributes:\")\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'int':\n",
    "        print(f\"{column} is a discrete attribute.\")\n",
    "    elif df[column].dtype == 'float':\n",
    "        print(f\"{column} is a continuous attribute.\")\n",
    "\n",
    "# Perform basic operations on continuous attributes\n",
    "df['ScaledIncome'] = df['Income'] / 1000  # Scale income for readability\n",
    "\n",
    "# Display the dataset after the operation\n",
    "print(\"\\nDataset after performing an operation on continuous attributes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "    We create a pandas DataFrame with three attributes: 'NumCourses' (discrete), 'GPA' (continuous), and 'Income' (continuous).\n",
    "    We explore the nature of each attribute based on its data type.\n",
    "    We perform a basic operation (scaling) on a continuous attribute ('Income') to create a new attribute, 'ScaledIncome'.\n",
    "    The resulting DataFrame (df) is displayed after these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Measuring the central tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring central tendency is a way to summarize a set of data by identifying the central or average value. There are three common measures of central tendency: mean, median, and mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean (Average): \n",
    "It is calculated by summing up all the values in a dataset and dividing the sum by the number of values.\n",
    "\n",
    "#### Median: \n",
    "The median is the middle value of a dataset when it is sorted in ascending or descending order. If the dataset has an even number of values, the median is the average of the two middle values.\n",
    "\n",
    "#### Mode: \n",
    "The mode is the value that appears most frequently in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Measuring the central tendency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([15, 18, 2, 36, 12, 25, 18, 40, 28, 22])\n",
    "\n",
    "# Mean\n",
    "mean_value = np.mean(data)\n",
    "print(f\"Mean: {mean_value}\")\n",
    "\n",
    "# Median\n",
    "median_value = np.median(data)\n",
    "print(f\"Median: {median_value}\")\n",
    "\n",
    "# Mode\n",
    "mode_value = np.argmax(np.bincount(data))\n",
    "print(f\"Mode: {mode_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Make sure to install NumPy if you haven't already by running pip install numpy in your Python environment.\n",
    "\n",
    "    This code uses NumPy functions to calculate the mean, median, and mode of the given dataset. You can replace the data array with your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Measuring the dispersion of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the dispersion of data is important in understanding how spread out or clustered the values in a dataset are. There are several measures of dispersion, including range, variance, and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range: \n",
    "The range is the difference between the maximum and minimum values in a dataset. It provides a simple measure of how spread out the values are.\n",
    "\n",
    "#### Variance: \n",
    "Variance measures the average squared difference of each value from the mean of the dataset. A higher variance indicates greater dispersion.\n",
    "\n",
    "#### Standard Deviation: \n",
    "The standard deviation is the square root of the variance. It provides a more interpretable measure of the spread of data, as it is in the same unit as the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Measuring the dispersion of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([15, 18, 2, 36, 12, 25, 18, 40, 28, 22])\n",
    "\n",
    "# Range\n",
    "data_range = np.ptp(data)\n",
    "print(f\"Range: {data_range}\")\n",
    "\n",
    "# Variance\n",
    "data_variance = np.var(data)\n",
    "print(f\"Variance: {data_variance}\")\n",
    "\n",
    "# Standard Deviation\n",
    "data_stddev = np.std(data)\n",
    "print(f\"Standard Deviation: {data_stddev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this code, np.ptp calculates the range, np.var calculates the variance, and np.std calculates the standard deviation. Replace the data array with your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Covariance and correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance and correlation analysis are statistical measures that describe the degree to which two variables change together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance:\n",
    "\n",
    "Covariance measures the extent to which the values of two variables change in relation to each other.\n",
    "A positive covariance indicates that as one variable increases, the other variable tends to increase as well, and vice versa for negative covariance.\n",
    "However, the scale of covariance is not standardized, making it challenging to interpret the strength of the relationship.\n",
    "\n",
    "#### Correlation:\n",
    "\n",
    "Correlation is a standardized measure of the strength and direction of the linear relationship between two variables.\n",
    "The correlation coefficient ranges from -1 to 1.\n",
    "A correlation coefficient of 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Covariance and correlation analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Variable1': [15, 18, 2, 36, 12, 25, 18, 40, 28, 22],\n",
    "    'Variable2': [10, 12, 5, 30, 8, 20, 15, 35, 25, 18]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(df, rowvar=False)\n",
    "print(f\"Covariance Matrix:\\n{cov_matrix}\")\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(f\"Correlation Matrix:\\n{correlation_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This code calculates the covariance matrix using np.cov and the correlation matrix using the corr method of a Pandas DataFrame. Replace the Variable1 and Variable2 columns with your own variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Graphic displays of basic statistics of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphic displays of basic statistics are essential for visualizing the characteristics of a dataset. These displays can help in gaining insights into the distribution, central tendency, and dispersion of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms:\n",
    "\n",
    "Explanation: Histograms provide a visual representation of the distribution of a dataset by dividing it into bins and displaying the frequency or probability of values falling into each bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plots:\n",
    "\n",
    "Explanation: Box plots (box-and-whisker plots) provide a graphical summary of the distribution of a dataset, including the median, quartiles, and potential outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plots:\n",
    "\n",
    "Explanation: Scatter plots display individual data points in a two-dimensional space, making it easy to identify patterns, relationships, and outliers between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Python Code for Graphic displays of basic statistics of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.random.randn(1000)  # Replace with your own dataset\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(data, bins=20, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of the Dataset')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.random.randn(1000)  # Replace with your own dataset\n",
    "\n",
    "# Create a box plot\n",
    "plt.boxplot(data, vert=False)\n",
    "plt.title('Box Plot of the Dataset')\n",
    "plt.xlabel('Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example datasets\n",
    "x = np.random.randn(100)\n",
    "y = 2 * x + np.random.randn(100)  # Replace with your own datasets\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y, color='red', alpha=0.7)\n",
    "plt.title('Scatter Plot of Two Variables')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Histograms, box plots, and scatter plots can enhance the understanding of data distributions, relationships, and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Data matrix vs. dissimilarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data mining, understanding the concepts of data matrix and dissimilarity matrix is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Matrix:\n",
    "\n",
    "Explanation: A data matrix is a structured representation of a dataset, where rows correspond to individual observations or instances, and columns represent different attributes or features. Each cell in the matrix contains the value of a specific attribute for a particular instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dissimilarity Matrix:\n",
    "\n",
    "Explanation: A dissimilarity matrix represents the dissimilarity or similarity between pairs of instances in a dataset. It quantifies how different or similar two instances are based on some measure of dissimilarity, such as distance or dissimilarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data matrix\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Feature1': [10, 15, 20, 25],\n",
    "    'Feature2': [0.5, 1.2, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Data Matrix:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissimilarity Matrix\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "\n",
    "# Example data matrix\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Feature1': [10, 15, 20, 25],\n",
    "    'Feature2': [0.5, 1.2, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Euclidean distance matrix\n",
    "dissimilarity_matrix = squareform(pdist(df[['Feature1', 'Feature2']], metric='euclidean'))\n",
    "print(\"Dissimilarity Matrix:\")\n",
    "print(pd.DataFrame(dissimilarity_matrix, index=df['ID'], columns=df['ID']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the pdist function from SciPy calculates the pairwise Euclidean distances between instances based on selected features. The squareform function converts the condensed distance matrix to a square form.\n",
    "\n",
    "    Matrices provide a structured representation of raw data, while dissimilarity matrices capture the pairwise dissimilarities between instances, which is useful in various data mining applications like clustering, classification, and similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Proximity measures for nominal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for nominal attributes are used to quantify the similarity or dissimilarity between instances when dealing with categorical or nominal attributes. Unlike numerical attributes, nominal attributes don't have a natural ordering, making traditional distance metrics unsuitable. Proximity measures address this issue by focusing on the agreement or disagreement between categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Jaccard Similarity\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Jaccard Similarity measures the similarity between two sets by dividing the size of their intersection by the size of their union. In the context of nominal attributes, each attribute value is treated as a set of binary values (1 if the attribute is present, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Example dataset with nominal attributes\n",
    "data = {\n",
    "    'Instance1': ['Red', 'Square', 'Large'],\n",
    "    'Instance2': ['Blue', 'Circle', 'Small']\n",
    "}\n",
    "\n",
    "# Convert nominal attributes to binary values\n",
    "binary_data = pd.get_dummies(pd.DataFrame(data))\n",
    "binary_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Jaccard Similarity\n",
    "jaccard_similarity = jaccard_score(binary_data.iloc[0], binary_data.iloc[1])\n",
    "jaccard_similarity_1 = jaccard_score(binary_data.iloc[0], binary_data.iloc[0])\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the nominal attributes ('Red', 'Square', 'Large') and ('Blue', 'Circle', 'Small') are converted into binary vectors using one-hot encoding. The Jaccard Similarity is then calculated using the jaccard_score function from scikit-learn.\n",
    "\n",
    "    Proximity measures for nominal attributes are essential for clustering, classification, and other data mining tasks involving categorical data. The practical example illustrates the application of Jaccard Similarity to quantify the similarity between instances with nominal attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Proximity measures for binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for binary attributes are employed when dealing with datasets that consist of binary (0/1) attributes. These measures assess the similarity or dissimilarity between instances based on the presence or absence of specific binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Hamming Distance\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Hamming Distance measures the dissimilarity between two binary strings by counting the number of positions at which the corresponding bits are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "# Example dataset with binary attributes\n",
    "data = {\n",
    "    'Instance1': [1, 0, 1, 0],\n",
    "    'Instance2': [0, 1, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert binary attributes to NumPy arrays\n",
    "binary_data = np.array([data['Instance1'], data['Instance2']])\n",
    "\n",
    "# Calculate Hamming Distance\n",
    "hamming_distance = hamming(binary_data[0], binary_data[1])\n",
    "print(f\"Hamming Distance: {hamming_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, instances are represented by binary vectors [1, 0, 1, 0] and [0, 1, 1, 0]. The Hamming Distance between these vectors is computed using the hamming function from SciPy.\n",
    "\n",
    "    Proximity measures for binary attributes are crucial for tasks such as clustering, pattern recognition, and classification when dealing with datasets consisting of binary features. The Hamming Distance example demonstrates how to quantify dissimilarity between instances based on binary attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Dissimilarity of numeric data: Minkowski distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minkowski distance is a dissimilarity measure used to assess the similarity or dissimilarity between two points in a multidimensional space. It is a generalization of other distance measures, including Euclidean distance and Manhattan distance, and is defined by the following formula:\n",
    "\n",
    "D(x,y)=(∑i=1n∣xi−yi∣p)1p\n",
    "\n",
    "where x and y are vectors representing the numeric data points, n is the number of dimensions, and p is a parameter that determines the order of the Minkowski distance. When p=1, the Minkowski distance is equivalent to the Manhattan distance, and when p=2, it is equivalent to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "# Example dataset with numeric attributes\n",
    "data1 = [2, 3, 5, 7]\n",
    "data2 = [1, 4, 6, 8]\n",
    "\n",
    "# Calculate Minkowski distance with p=2 (Euclidean distance)\n",
    "minkowski_distance = minkowski(data1, data2, p=2)\n",
    "print(f\"Minkowski Distance (p=2): {minkowski_distance}\")\n",
    "\n",
    "# Calculate Minkowski distance with p=1 (Manhattan distance)\n",
    "manhattan_distance = minkowski(data1, data2, p=1)\n",
    "print(f\"Minkowski Distance (p=1): {manhattan_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Minkowski distance is calculated between two numeric vectors [2, 3, 5, 7] and [1, 4, 6, 8] using both the Euclidean distance (p=2) and the Manhattan distance (p=1). The minkowski function from SciPy is used for the calculation.\n",
    "\n",
    "    The Minkowski distance is a flexible measure that adapts to different scenarios based on the chosen value of pp. It is applicable in various data mining tasks such as clustering, classification, and outlier detection, especially when dealing with datasets containing numeric attributes. The"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Proximity measures for ordinal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity measures for ordinal attributes are used when dealing with datasets that contain attributes with an inherent order or ranking. Unlike nominal attributes, ordinal attributes have a meaningful order, but the intervals between values may not be uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Proximity Measure: Spearman Rank Correlation Coefficient\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "The Spearman Rank Correlation Coefficient measures the strength and direction of the monotonic relationship between two ordinal variables. It is based on the ranks of the values rather than their actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Example dataset with ordinal attributes\n",
    "data = {\n",
    "    'Instance1': [2, 3, 1, 4],\n",
    "    'Instance2': [1, 4, 2, 3]\n",
    "}\n",
    "\n",
    "# Calculate Spearman Rank Correlation Coefficient\n",
    "spearman_corr, _ = spearmanr(data['Instance1'], data['Instance2'])\n",
    "print(f\"Spearman Rank Correlation Coefficient: {spearman_corr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, instances are represented by ordinal attributes [2, 3, 1, 4] and [1, 4, 2, 3]. The Spearman Rank Correlation Coefficient is calculated using the spearmanr function from SciPy.\n",
    "\n",
    "    Proximity measures for ordinal attributes are important when dealing with data that has a meaningful order but lacks a clear numerical scale. The Spearman Rank Correlation Coefficient is particularly useful for assessing the monotonic relationship between ordinal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6. Dissimilarity for attributes of mixed types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling dissimilarity for attributes of mixed types is a common challenge in data mining, as datasets often include a combination of numeric, categorical, and ordinal attributes. It's crucial to employ appropriate dissimilarity measures that can accommodate the diverse nature of these attribute types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Dissimilarity Measure: Gower's Distance\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "Gower's Distance is a dissimilarity measure designed for datasets with mixed types of attributes. It calculates the dissimilarity between two instances by considering the attribute types and applying appropriate measures, such as Euclidean distance for numeric attributes, Jaccard similarity for binary attributes, and simple matching coefficient for nominal attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset with mixed attribute types\n",
    "data = {\n",
    "    'Numeric': [2.5, 1.8, 3.2, 4.0],\n",
    "    'Binary': [1, 0, 1, 1],\n",
    "    'Nominal': ['A', 'B', 'A', 'C'],\n",
    "    'Ordinal': [2, 1, 3, 2]\n",
    "}\n",
    "\n",
    "# Standardize numeric attributes\n",
    "numeric_data = np.array([data['Numeric']]).T\n",
    "numeric_data = StandardScaler().fit_transform(numeric_data)\n",
    "\n",
    "# Convert binary attributes to binary values\n",
    "binary_data = pd.get_dummies(pd.DataFrame(data['Binary'], columns=['Binary']))\n",
    "\n",
    "# Convert nominal attributes to binary values\n",
    "nominal_data = pd.get_dummies(pd.DataFrame(data['Nominal'], columns=['Nominal']))\n",
    "\n",
    "# Combine all attribute types\n",
    "mixed_data = np.concatenate((numeric_data, binary_data, nominal_data, np.array([data['Ordinal']]).T), axis=1)\n",
    "\n",
    "# Calculate Gower's Distance\n",
    "gower_distance = squareform(pdist(mixed_data, metric='euclidean'))\n",
    "print(\"Gower's Distance Matrix:\")\n",
    "print(gower_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Gower's Distance is calculated for a dataset with numeric, binary, nominal, and ordinal attributes. Numeric attributes are standardized, binary and nominal attributes are converted to binary values, and all attribute types are combined into a single matrix. The pdist function from SciPy is then used to calculate pairwise Euclidean distances, and the squareform function converts the condensed distance matrix to a square form.\n",
    "\n",
    "    The Gower's Distance is a versatile measure for dissimilarity in datasets with mixed attribute types, providing a comprehensive solution for handling various data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is a metric used to measure the similarity between two non-zero vectors of an inner product space. In the context of data mining, it is often employed to assess the similarity between documents in natural language processing or to compare feature vectors in recommendation systems. The cosine similarity ranges from -1 to 1, where 1 indicates perfect similarity, 0 indicates no similarity, and -1 indicates perfect dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example dataset with text documents\n",
    "documents = [\n",
    "    \"Data mining is an exciting field of study.\",\n",
    "    \"Machine learning techniques are used for data analysis.\",\n",
    "    \"Python programming is widely used in data science applications.\"\n",
    "]\n",
    "\n",
    "# Convert documents to a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "document_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(document_matrix, document_matrix)\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the CountVectorizer from scikit-learn is used to convert a collection of text documents into a matrix of token counts (bag-of-words representation). The cosine_similarity function then computes the cosine similarity between the document vectors.\n",
    "\n",
    "    Cosine similarity is particularly useful in scenarios where the magnitude of the vectors is not crucial, and the focus is on the direction of the vectors. It is widely applied in text analysis, document retrieval, and collaborative filtering systems. The provided example demonstrates how to calculate cosine similarity for a set of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8. Measuring similar distributions: the Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler (KL) Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In the context of data mining, it is often used to quantify the difference or similarity between two probability distributions. KL Divergence is not a true distance metric as it is not symmetric and does not satisfy the triangle inequality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Example probability distributions\n",
    "distribution1 = np.array([0.2, 0.3, 0.5])\n",
    "distribution2 = np.array([0.1, 0.6, 0.3])\n",
    "\n",
    "# Calculate Kullback-Leibler Divergence\n",
    "kl_divergence = entropy(distribution1, distribution2)\n",
    "print(f\"Kullback-Leibler Divergence: {kl_divergence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, two probability distributions are represented by arrays distribution1 and distribution2. The entropy function from SciPy calculates the KL Divergence between these distributions.\n",
    "\n",
    "    KL Divergence is often used in information theory and statistics to measure the difference between two probability distributions. It finds applications in various areas, including natural language processing, machine learning, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9. Capturing hidden semantics in similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing hidden semantics in similarity measures involves finding meaningful patterns, relationships, or similarities in data that may not be apparent at first glance. This process often requires more advanced techniques that go beyond simple distance or similarity metrics. Methods like word embeddings, semantic similarity, or advanced neural network models are employed to capture and leverage hidden semantics in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example dataset with text documents\n",
    "documents = [\n",
    "    \"Data mining is an exciting field of study.\",\n",
    "    \"Machine learning techniques are used for data analysis.\",\n",
    "    \"Python programming is widely used in data science applications.\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents into words\n",
    "tokenized_documents = [doc.split() for doc in documents]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized documents\n",
    "model = Word2Vec(tokenized_documents, vector_size=10, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Calculate cosine similarity using word embeddings\n",
    "embedding_similarity = cosine_similarity([model.wv['data']], [model.wv['mining']])\n",
    "print(f\"Cosine Similarity using Word Embeddings: {embedding_similarity[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Gensim library is used to train a Word2Vec model on a small dataset of text documents. Word embeddings capture semantic relationships between words, and cosine similarity is then used to measure the similarity between the word vectors of two words ('data' and 'mining').\n",
    "\n",
    "    Capturing hidden semantics often involves leveraging advanced techniques such as word embeddings, neural networks, or deep learning models. These methods are particularly useful in applications like natural language processing, recommendation systems, and image analysis where capturing the underlying meaning or semantics is crucial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
