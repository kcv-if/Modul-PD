{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer to E-Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Overview of basic clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis is a fundamental technique in data mining that involves grouping similar data points together into clusters or groups. It helps in identifying patterns and structures within datasets, facilitating insights into the inherent organization of the data.\n",
    "\n",
    "This section explores the core concepts and methods of cluster analysis, emphasizing the techniques used to partition data into meaningful groups.\n",
    "\n",
    "Various clustering methods exist, each with its strengths and suitable scenarios. Common clustering methods include K-Means, Hierarchical Clustering, and DBSCAN (Density-Based Spatial Clustering of Applications with Noise). Each method uses different approaches to define clusters based on the characteristics of the data.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of K-Means clustering using the famous Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_standardized)\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "iris_clustered = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "iris_clustered['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(iris_clustered['sepal length (cm)'], iris_clustered['sepal width (cm)'], c=iris_clustered['Cluster'], cmap='viridis')\n",
    "plt.title('K-Means Clustering of Iris Dataset')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the K-Means clustering algorithm to cluster the Iris dataset based on sepal length and sepal width. The data is standardized to ensure that all features have the same scale. The resulting clusters are visualized in a scatter plot, with each point colored according to its assigned cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. k-Means: a centroid-based technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning methods involve dividing data points into clusters where each point belongs to only one cluster. One of the widely used partitioning techniques is k-Means.\n",
    "\n",
    "k-Means is a centroid-based clustering algorithm. It aims to partition the data into k clusters, where each cluster is represented by its centroid. The algorithm iteratively assigns data points to the nearest centroid and updates the centroids based on the assigned points.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of k-Means clustering using a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Visualize the clusters and centroids\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('k-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the make_blobs function from scikit-learn to create a synthetic dataset with three clusters. The K-Means algorithm is then applied to cluster the data into three groups. The resulting clusters and centroids are visualized in a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Variations of k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several variations of the k-Means algorithm exist, each designed to address specific challenges or improve performance. Examples include K-Means++, Mini-Batch K-Means, and the use of different distance metrics.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of Mini-Batch K-Means using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply Mini-Batch K-Means clustering\n",
    "mbk = MiniBatchKMeans(n_clusters=3, batch_size=50, random_state=42)\n",
    "mbk.fit(X_standardized)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=mbk.labels_, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.scatter(mbk.cluster_centers_[:, 0], mbk.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('Mini-Batch K-Means Clustering of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use Mini-Batch K-Means to cluster the Iris dataset. The data is standardized, and dimensionality is reduced using PCA for visualization. Mini-Batch K-Means is a variation of the classic algorithm that processes small batches of data at each iteration, making it more scalable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Agglomerative hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative Hierarchical Clustering starts with each data point as a separate cluster and iteratively merges the closest pairs of clusters until only one cluster remains. The process is governed by a linkage criterion, such as Ward's method or average linkage.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's consider a practical example of Agglomerative Hierarchical Clustering using the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply Agglomerative Hierarchical Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Create a linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Visualize the dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=3, show_leaf_counts=False, no_labels=True)\n",
    "plt.title('Dendrogram for Agglomerative Hierarchical Clustering')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, Agglomerative Hierarchical Clustering is applied to the Iris dataset. The resulting dendrogram visually represents the hierarchy of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. Divisive hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisive Hierarchical Clustering starts with all data points in a single cluster and recursively divides the cluster into sub-clusters. The process continues until each data point is in its cluster. This approach requires defining a criterion for splitting clusters.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Divisive Hierarchical Clustering is less common than Agglomerative Hierarchical Clustering in practice, and scikit-learn does not provide a specific implementation for divisive clustering. However, you can achieve divisive clustering using agglomerative clustering and manipulating the dendrogram. Here's a simplified example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Apply hierarchical clustering\n",
    "labels = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "\n",
    "# Visualize the dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=3, show_leaf_counts=False, no_labels=True)\n",
    "plt.title('Dendrogram for Divisive Hierarchical Clustering')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('Divisive Hierarchical Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use hierarchical clustering with the fcluster function to perform divisive clustering on the Iris dataset. The dendrogram shows the hierarchy of clusters, and the scatter plot visualizes the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.4. BIRCH: scalable hierarchical clustering using clustering feature trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRCH is a hierarchical clustering algorithm that utilizes a tree structure to efficiently organize and represent clusters. It employs a Clustering Feature Tree (CF Tree) to summarize the data and iteratively refines clusters in a balanced and memory-efficient manner.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "BIRCH is available in scikit-learn. Let's use it on a synthetic dataset for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply BIRCH clustering\n",
    "birch = Birch(threshold=0.5, n_clusters=3)\n",
    "birch_labels = birch.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=birch_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('BIRCH Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the BIRCH algorithm to cluster a synthetic dataset with three clusters. The threshold parameter controls the compactness of subclusters within a CF Tree. Adjust the parameters based on your specific use case and dataset. BIRCH is particularly useful for large datasets where memory efficiency is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.5. Probabilistic hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic hierarchical clustering involves using probabilistic models to describe the uncertainty or likelihood of data points belonging to different clusters in a hierarchical structure. Gaussian Mixture Models (GMMs) are an example of probabilistic clustering models.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's use Gaussian Mixture Models (GMMs) as an example of probabilistic hierarchical clustering. GMMs allow data points to belong to multiple clusters with varying probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X_standardized)\n",
    "gmm_labels = gmm.predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters with probabilities\n",
    "probs = gmm.predict_proba(X_standardized)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('Probabilistic Hierarchical Clustering (GMM)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot ellipses representing cluster shapes based on covariances\n",
    "for i in range(3):\n",
    "    plt.contour(X[:, 0], X[:, 1], probs[:, i], colors='black', linewidths=0.5, levels=[0.1, 0.5, 0.9])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use a Gaussian Mixture Model (GMM) for probabilistic clustering on a synthetic dataset. The clusters are represented with varying probabilities, and ellipses depict the shapes of the clusters based on the covariance matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1. DBSCAN: density-based clustering based on connected regions with high density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is a density-based clustering algorithm that groups together data points that are close to each other and have a sufficient number of neighboring points, forming dense regions. It also identifies noise points that do not belong to any cluster.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "Let's use DBSCAN to cluster the Iris dataset, which has distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('DBSCAN Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use DBSCAN to cluster the Iris dataset. The eps parameter controls the maximum distance between two samples for one to be considered as part of the neighborhood, and min_samples sets the minimum number of samples required to form a dense region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2. DENCLUE: clustering based on density distribution functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DENCLUE is a density-based clustering algorithm that represents clusters using density distribution functions. It identifies clusters as regions of high density in the feature space, allowing for more flexible cluster shapes compared to traditional methods.\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "DENCLUE is not as widely used as some other clustering algorithms, and it might not be available in standard machine learning libraries like scikit-learn. However, you can implement it using third-party libraries. Here's a simplified example using the DENCLUE implementation from the hdbscan library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DENCLUE clustering\n",
    "denclue = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True)\n",
    "denclue_labels = denclue.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=denclue_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('DENCLUE Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the hdbscan library, which provides an implementation of DENCLUE through the HDBSCAN class. The min_cluster_size parameter controls the minimum number of samples required to form a dense region. Adjust parameters based on your specific use case and dataset. Note that for a full implementation of DENCLUE, you might need to refer to specific research papers or implement it from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3. Grid-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid-based methods divide the data space into a grid structure, creating cells or bins. Clustering is then performed based on the distribution of data points within these cells. A popular example is STING (Statistical Information Grid).\n",
    "\n",
    "#### Real-world Example in Python:\n",
    "\n",
    "For a practical example, we can use the K-Means algorithm, which, while not strictly grid-based, can be applied to grid-like structures. Here, we'll create a synthetic dataset with a grid-like structure and apply K-Means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset with a grid-like structure\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_standardized)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('K-Means Clustering on Grid-Like Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a synthetic dataset with a grid-like structure using the make_blobs function. K-Means clustering is then applied to the standardized data. Adjust the dataset characteristics and K-Means parameters based on your specific use case. While this is not a grid-based method in the strict sense, it showcases how clustering algorithms can be applied to datasets with grid-like patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1. Assessing clustering tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing clustering tendency is a crucial step in cluster analysis, as it helps determine whether meaningful clusters exist in the dataset. This process involves evaluating whether the data naturally forms distinct groups or if it is randomly distributed. Two common methods for assessing clustering tendency are:\n",
    "\n",
    "- Hopkins Statistic:\n",
    "    The Hopkins statistic measures the tendency of a dataset to form clusters. It compares the distance distribution between randomly selected points and the actual data points. A low Hopkins score indicates a higher likelihood of clustering.\n",
    "\n",
    "- Silhouette Score:\n",
    "    The silhouette score quantifies how well-separated clusters are. It ranges from -1 to 1, where a high positive value indicates well-defined clusters, a score around 0 suggests overlapping clusters, and negative values imply incorrect clustering.\n",
    "\n",
    "These methods aid in deciding whether to proceed with cluster analysis and which clustering algorithm may be most appropriate for the dataset.\n",
    "\n",
    "#### Practical use in Python for assessing clustering tendency using the Hopkins statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the Hopkins statistic\n",
    "def hopkins_statistic(X):\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Create synthetic data for comparison\n",
    "    synthetic_data = np.random.rand(n, d)\n",
    "\n",
    "    # Calculate distance between real data points\n",
    "    real_data_distances = NearestNeighbors(n_neighbors=1).fit(X).kneighbors(X)[0].ravel()\n",
    "\n",
    "    # Calculate distance between synthetic data points\n",
    "    synthetic_data_distances = NearestNeighbors(n_neighbors=1).fit(synthetic_data).kneighbors(synthetic_data)[0].ravel()\n",
    "\n",
    "    # Calculate Hopkins Statistic\n",
    "    hopkins_stat = sum(real_data_distances) / (sum(real_data_distances) + sum(synthetic_data_distances))\n",
    "\n",
    "    return hopkins_stat\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Calculate Hopkins Statistic\n",
    "hopkins_score = hopkins_statistic(X)\n",
    "\n",
    "print(f\"Hopkins Statistic: {hopkins_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, standardize it, and then calculate the Hopkins Statistic using the hopkins_statistic function. The result gives an indication of the clustering tendency of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2. Determining the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters is a critical aspect of cluster analysis. A variety of methods can be employed for this purpose, and some common approaches include:\n",
    "\n",
    "- Elbow Method:\n",
    "        The Elbow Method involves plotting the explained variation as a function of the number of clusters and looking for the \"elbow\" point. The point where the rate of decrease sharply changes may indicate the optimal number of clusters.\n",
    "\n",
    "- Silhouette Analysis:\n",
    "        Silhouette analysis assesses how well-separated clusters are. It calculates a silhouette score for different numbers of clusters, and a higher score suggests a better-defined clustering.\n",
    "\n",
    "- Gap Statistics:\n",
    "        Gap statistics compare the clustering quality of the dataset with that of a random distribution. A larger gap statistic indicates a more appropriate number of clusters.\n",
    "\n",
    "Selecting the right method depends on the characteristics of the dataset and the desired outcome of the analysis.\n",
    "\n",
    "#### Real-world example of practical use in Python for determining the number of clusters using the Elbow Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the distortion (inertia) for a range of clusters\n",
    "def calculate_distortion(X, max_clusters):\n",
    "    distortions = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    return distortions\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Calculate distortions for a range of clusters\n",
    "max_clusters = 10\n",
    "distortions = calculate_distortion(X, max_clusters)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_clusters + 1), distortions, marker='o')\n",
    "plt.title('Elbow Method for Optimal Cluster Number')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Distortion (Inertia)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, and then apply the Elbow Method to find the optimal number of clusters. The \"elbow\" point in the graph indicates the suggested number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3. Measuring clustering quality: extrinsic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrinsic methods for measuring clustering quality involve evaluating the performance of clustering algorithms using external information, such as ground truth labels or known class memberships. Common extrinsic metrics include:\n",
    "\n",
    "1. Adjusted Rand Index (ARI):\n",
    "        ARI measures the similarity between the true labels and the labels assigned by the clustering algorithm. It considers all pairs of samples and assesses whether they are placed in the same or different clusters, providing a score between -1 and 1.\n",
    "\n",
    "2. Normalized Mutual Information (NMI):\n",
    "        NMI quantifies the mutual information between the true labels and the predicted labels, normalized by the entropy of the labels. Like ARI, NMI ranges from 0 to 1, with higher values indicating better clustering quality.\n",
    "\n",
    "These metrics help assess how well a clustering algorithm aligns with the known structure of the data, providing insight into its effectiveness in capturing meaningful patterns.\n",
    "\n",
    "#### Example of practical use in Python for measuring clustering quality using the Adjusted Rand Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data with ground truth labels (replace this with your dataset)\n",
    "X, true_labels = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply a clustering algorithm (KMeans in this case)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate Adjusted Rand Index\n",
    "ari_score = adjusted_rand_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with ground truth labels using make_blobs from scikit-learn, apply the KMeans clustering algorithm, and calculate the Adjusted Rand Index to measure the clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4. Intrinsic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic methods for measuring clustering quality assess the performance of clustering algorithms based solely on the internal structure of the data, without relying on external information. Common intrinsic metrics include:\n",
    "\n",
    "- Silhouette Score:\n",
    "        The silhouette score quantifies how well-separated clusters are. It is calculated for each data point and ranges from -1 to 1. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "- Davies-Bouldin Index:\n",
    "        The Davies-Bouldin Index measures the compactness and separation between clusters. A lower Davies-Bouldin Index suggests better clustering, with minimal intra-cluster distance and maximal inter-cluster distance.\n",
    "\n",
    "- Calinski-Harabasz Index:\n",
    "        The Calinski-Harabasz Index evaluates the ratio of between-cluster variance to within-cluster variance. A higher index indicates better-defined clusters.\n",
    "\n",
    "These metrics provide insights into the inherent quality of the clusters formed by the algorithm, without requiring external validation.\n",
    "\n",
    "#### A real-world example of practical use in Python for measuring clustering quality using the Silhouette Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply KMeans clustering with different numbers of clusters\n",
    "cluster_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Plot the Silhouette Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score for Optimal Cluster Number')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn, apply KMeans clustering with different numbers of clusters, and plot the Silhouette Scores. The number of clusters with the highest silhouette score can be considered optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1. Fuzzy clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy clustering is an extension of traditional clustering methods that allows data points to belong to multiple clusters simultaneously, each with a degree of membership. Unlike hard clustering, where a point exclusively belongs to a single cluster, fuzzy clustering introduces a degree of uncertainty. The primary model used for fuzzy clustering is the fuzzy c-means (FCM) algorithm.\n",
    "\n",
    "In FCM, each data point has membership values across all clusters, represented as a fuzzy membership matrix. These membership values range between 0 and 1, indicating the likelihood of a data point belonging to a specific cluster. Fuzzy clustering is particularly useful when data points exhibit partial membership to multiple clusters.\n",
    "\n",
    "#### A real-world example of practical use in Python for fuzzy clustering using the Fuzzy C-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Fuzzy C-Means clustering\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "def fuzzy_c_means(X, n_clusters, m=2, max_iter=100, tol=1e-4):\n",
    "    # Initialize cluster centers using K-Medoids\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters)\n",
    "    kmedoids.fit(X)\n",
    "    centers = kmedoids.cluster_centers_\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate distances and update membership matrix\n",
    "        distances = pairwise_distances_argmin_min(X, centers)[1]\n",
    "        membership_matrix = 1 / distances[:, None] ** (2 / (m - 1))\n",
    "        membership_matrix = membership_matrix / np.sum(membership_matrix, axis=1)[:, None]\n",
    "\n",
    "        # Update cluster centers\n",
    "        new_centers = np.dot(membership_matrix.T, X) / np.sum(membership_matrix, axis=0)[:, None]\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_centers - centers) < tol:\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    # Assign final clusters based on maximum membership\n",
    "    labels = np.argmax(membership_matrix, axis=1)\n",
    "    return labels, centers\n",
    "\n",
    "# Apply Fuzzy C-Means clustering\n",
    "n_clusters = 3\n",
    "fuzzy_labels, fuzzy_centers = fuzzy_c_means(X, n_clusters)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=fuzzy_labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(fuzzy_centers[:, 0], fuzzy_centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Fuzzy C-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Fuzzy C-Means algorithm using a custom implementation. The resulting fuzzy clusters and cluster centers are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2. Probabilistic model-based clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic model-based clustering involves the use of statistical models to describe the underlying distribution of data and identify clusters based on the parameters of these models. One common approach is the Gaussian Mixture Model (GMM), where data points are assumed to be generated from a mixture of several Gaussian distributions.\n",
    "\n",
    "In GMM-based clustering, each cluster is associated with a Gaussian distribution characterized by its mean, covariance matrix, and weight. The model allows for more flexibility in capturing complex cluster shapes and provides a probability distribution over cluster assignments. The Expectation-Maximization (EM) algorithm is often employed for parameter estimation in GMMs.\n",
    "\n",
    "#### A real-world example of practical use in Python for probabilistic model-based clustering using the Gaussian Mixture Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering\n",
    "n_components = 3\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gmm.fit(X)\n",
    "probabilities = gmm.predict_proba(X)  # Probabilities of each sample belonging to each cluster\n",
    "labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Gaussian Mixture Model Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Gaussian Mixture Model for clustering. The resulting probabilistic model-based clusters and cluster centers are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3. Expectation-maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expectation-Maximization (EM) algorithm is a general framework for finding maximum likelihood estimates of parameters in probabilistic models with latent variables. In the context of probabilistic model-based clustering, the EM algorithm is often used for parameter estimation in Gaussian Mixture Models (GMMs).\n",
    "\n",
    "Here's a brief overview of the EM algorithm in the context of GMM-based clustering:\n",
    "\n",
    "- Expectation Step (E-step):\n",
    "    Compute the expected value of the latent variables (cluster assignments) given the observed data and the current parameter estimates.\n",
    "\n",
    "- Maximization Step (M-step):\n",
    "    Update the parameters (mean, covariance, and weights) of the Gaussian distributions based on the observed data and the expected values obtained in the E-step.\n",
    "\n",
    "- Iteration:\n",
    "    Repeat the E-step and M-step until convergence, where the change in the log-likelihood or parameters is below a predefined threshold.\n",
    "\n",
    "The EM algorithm iteratively refines the parameter estimates, maximizing the likelihood of the observed data given the model.\n",
    "\n",
    "#### Real-world example of practical use in Python for the Expectation-Maximization algorithm applied to Gaussian Mixture Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Gaussian Mixture Model clustering with the Expectation-Maximization algorithm\n",
    "n_components = 3\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('Gaussian Mixture Model Clustering with EM Algorithm')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data using make_blobs from scikit-learn and then apply the Gaussian Mixture Model with the EM algorithm for clustering. The resulting clusters and cluster centers are visualized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Axis-parallel subspace approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering high-dimensional data poses unique challenges, such as the curse of dimensionality and the presence of irrelevant or redundant features. Axis-parallel subspace approaches aim to address these challenges by identifying subspaces within the high-dimensional feature space where clusters are more apparent.\n",
    "\n",
    "Key characteristics of axis-parallel subspace approaches include:\n",
    "\n",
    "1. Feature Subset Selection:\n",
    "        These methods involve selecting a subset of features or dimensions that are most informative for clustering, discarding irrelevant or redundant dimensions.\n",
    "\n",
    "2. Axis-Parallel Clustering:\n",
    "        Clustering is performed within the selected subspace using methods that are sensitive to the geometry of the data along each axis.\n",
    "\n",
    "3. Projection and Visualization:\n",
    "        Subspace clustering often includes projecting the data onto the selected subspace for better visualization and interpretation.\n",
    "\n",
    "Common techniques for axis-parallel subspace clustering include subspace clustering based on k-means or density-based clustering within subspaces.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for axis-parallel subspace clustering using the k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply KMeans clustering in the reduced subspace\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('KMeans Clustering in Reduced Subspace using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform KMeans clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. Arbitrarily oriented subspace approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrarily oriented subspace approaches address the challenges of clustering high-dimensional data by identifying subspaces that are not necessarily aligned with the axes of the original feature space. Unlike axis-parallel approaches, these methods consider subspaces with arbitrary orientations, allowing for more flexibility in capturing complex relationships within the data.\n",
    "\n",
    "Key characteristics of arbitrarily oriented subspace approaches include:\n",
    "\n",
    "- Subspace Definition:\n",
    "    These methods allow for the definition of subspaces based on linear combinations of original features, enabling the identification of clusters in non-axis-parallel subspaces.\n",
    "\n",
    "- Subspace Clustering Techniques:\n",
    "    Techniques such as subspace clustering based on affinity propagation or spectral clustering are often employed to identify clusters within the defined subspaces.\n",
    "\n",
    "- Robustness to Feature Redundancy:\n",
    "    Arbitrarily oriented subspace approaches are generally more robust to feature redundancy, making them suitable for high-dimensional datasets with correlated features.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for arbitrarily oriented subspace clustering using the Spectral Clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply Spectral Clustering in the reduced subspace\n",
    "spectral_clustering = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', random_state=42)\n",
    "labels = spectral_clustering.fit_predict(X)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering in Reduced Subspace using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform Spectral Clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2. Types of biclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biclustering is a technique that simultaneously clusters both rows and columns of a dataset, identifying submatrices that exhibit cohesive patterns. There are several types of biclusters based on the nature of the patterns they capture:\n",
    "\n",
    "- Constant Value Biclusters:\n",
    "    Biclusters where all the elements have a constant value, indicating uniformity within the identified submatrix.\n",
    "\n",
    "- Shift Biclusters:\n",
    "    Biclusters where the values within the submatrix exhibit a constant shift, preserving the overall pattern but allowing for variation.\n",
    "\n",
    "- Additive Biclusters:\n",
    "    Biclusters where the values can be modeled as a sum of a constant and a shift, capturing both constant and shifting patterns simultaneously.\n",
    "\n",
    "- Multiplicative Biclusters:\n",
    "    Biclusters where the values can be modeled as a product of a constant and a shift, capturing both constant and scaling patterns.\n",
    "\n",
    "Understanding the type of bicluster is essential for interpreting the biological or domain-specific meaning behind the patterns identified in the data.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for biclustering using the Bicluster class from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_checkerboard\n",
    "from sklearn.cluster import SpectralBiclustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with a checkerboard pattern\n",
    "data, _ = make_checkerboard(shape=(300, 300), n_clusters=(3, 3), noise=10, random_state=42)\n",
    "\n",
    "# Apply Spectral Biclustering\n",
    "model = SpectralBiclustering(n_clusters=(3, 3), random_state=42)\n",
    "model.fit(data)\n",
    "\n",
    "# Get row and column indices of the biclusters\n",
    "rows, cols = model.row_labels_, model.column_labels_\n",
    "\n",
    "# Plot the original and biclustered data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Data')\n",
    "plt.imshow(data, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Biclustered Data')\n",
    "plt.imshow(data[np.argsort(rows)], cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with a checkerboard pattern using make_checkerboard from scikit-learn and then apply Spectral Biclustering. The original and biclustered data are visualized side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3. Biclustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biclustering methods are designed to discover patterns in datasets where subsets of both rows and columns exhibit similar behavior. These methods provide a powerful tool for uncovering complex structures in various types of data. Some common biclustering algorithms include:\n",
    "\n",
    "- Spectral Biclustering:\n",
    "    This method uses eigenvalue decomposition to identify biclusters in the data. It considers both row and column relationships and is particularly effective for discovering additive biclusters.\n",
    "\n",
    "- Plaid Model Biclustering:\n",
    "    The Plaid Model represents the data as a superposition of additive biclusters. Biclusters are found by iteratively fitting row and column patterns.\n",
    "\n",
    "- Bimax Biclustering:\n",
    "    Bimax is a binary matrix factorization method that aims to find submatrices containing only 0s and 1s. It is suitable for binary data.\n",
    "\n",
    "- Order-Preserving Biclustering:\n",
    "    This method identifies biclusters by preserving the order of data points within rows and columns, capturing patterns based on monotonic relationships.\n",
    "\n",
    "Each biclustering method is designed to address specific characteristics of the data and the types of patterns one expects to find.\n",
    "\n",
    "#### Real-world example of practical use in Python for biclustering using the SpectralBiclustering class from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_checkerboard\n",
    "from sklearn.cluster import SpectralBiclustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with a checkerboard pattern\n",
    "data, _ = make_checkerboard(shape=(300, 300), n_clusters=(3, 3), noise=10, random_state=42)\n",
    "\n",
    "# Apply Spectral Biclustering\n",
    "model = SpectralBiclustering(n_clusters=(3, 3), random_state=42)\n",
    "model.fit(data)\n",
    "\n",
    "# Get row and column indices of the biclusters\n",
    "rows, cols = model.row_labels_, model.column_labels_\n",
    "\n",
    "# Plot the original and biclustered data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Data')\n",
    "plt.imshow(data, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Biclustered Data')\n",
    "plt.imshow(data[np.argsort(rows)], cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with a checkerboard pattern using make_checkerboard from scikit-learn and then apply Spectral Biclustering. The original and biclustered data are visualized side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1. Linear dimensionality reduction methods for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear dimensionality reduction methods aim to reduce the number of features in a dataset while preserving essential information. These techniques are particularly useful for clustering high-dimensional data, as they can enhance the efficiency and interpretability of clustering algorithms. Some common linear dimensionality reduction methods for clustering include:\n",
    "\n",
    "1. Principal Component Analysis (PCA):\n",
    "    PCA identifies a set of orthogonal axes (principal components) that capture the maximum variance in the data. It is effective for reducing dimensionality while preserving the overall structure.\n",
    "\n",
    "2. Linear Discriminant Analysis (LDA):\n",
    "    LDA aims to maximize the separation between different classes in the data. It is particularly useful for clustering when class information is available.\n",
    "\n",
    "3. Non-Negative Matrix Factorization (NMF):\n",
    "    NMF factorizes the data matrix into non-negative matrices, representing parts-based, interpretable features. It is suitable for data with non-negative values.\n",
    "\n",
    "4. Independent Component Analysis (ICA):\n",
    "    ICA seeks to find independent components in the data. It is useful for separating mixed signals and identifying underlying sources.\n",
    "\n",
    "These linear dimensionality reduction methods can be applied before clustering to reduce noise, improve computational efficiency, and enhance the quality of cluster assignments.\n",
    "\n",
    "#### A real-world example of practical use in Python for linear dimensionality reduction using PCA for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic high-dimensional data (replace this with your dataset)\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0, n_features=10)\n",
    "\n",
    "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply KMeans clustering in the reduced subspace\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results in the reduced subspace\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Cluster Centers')\n",
    "plt.title('KMeans Clustering after PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic high-dimensional data using make_blobs from scikit-learn, apply Principal Component Analysis (PCA) for dimensionality reduction, and then perform KMeans clustering in the reduced subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2. Nonnegative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonnegative Matrix Factorization (NMF) is a dimensionality reduction technique that factorizes a nonnegative data matrix into two lower-dimensional matrices, where all elements are nonnegative. One matrix represents the basis vectors (features), and the other represents the coefficients for each data point in terms of these basis vectors. NMF is particularly useful when the data has nonnegative values and when parts-based, interpretable features are desired.\n",
    "\n",
    "#### Key points about NMF:\n",
    "\n",
    "- Nonnegativity Constraint:\n",
    "    Both the basis vectors and coefficients in NMF are constrained to be nonnegative, which makes the resulting factors additive and interpretable.\n",
    "\n",
    "- Applications in Clustering:\n",
    "    NMF is often employed for clustering tasks, as it can reveal parts-based representations of the data, making it easier to interpret the underlying structure.\n",
    "\n",
    "- Suitability for Text and Image Data:\n",
    "    NMF is commonly used in natural language processing for topic modeling and in image analysis for extracting meaningful components.\n",
    "\n",
    "- Parameters:\n",
    "    The number of components in the factorization is a crucial parameter that determines the dimensionality of the reduced space.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for NMF applied to clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the 20 newsgroups dataset (replace this with your dataset)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer, NMF, and KMeans\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, nmf, kmeans)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(newsgroups.data)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_words = np.argsort(nmf.components_, axis=1)[:, :-11:-1]\n",
    "\n",
    "for i, topic in enumerate(top_words):\n",
    "    words = [feature_names[j] for j in topic]\n",
    "    print(f\"Topic {i + 1}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use NMF for dimensionality reduction in a pipeline that includes text vectorization (TfidfVectorizer), NMF, and KMeans clustering. This pipeline is applied to the 20 newsgroups dataset, and the top words for each topic are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3. Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Clustering is a technique that leverages the spectral properties of the data to perform clustering. It is particularly effective when dealing with non-convex clusters and complex geometric structures. Spectral Clustering involves the following steps:\n",
    "\n",
    "1. Graph Construction:\n",
    "        Construct an affinity matrix representing the pairwise similarity between data points. Common affinity measures include Gaussian similarity or nearest neighbors.\n",
    "\n",
    "2. Graph Transformation:\n",
    "        Transform the affinity matrix into a graph Laplacian matrix. The Laplacian matrix captures the connectivity and relationships between data points.\n",
    "\n",
    "3. Eigenvalue Decomposition:\n",
    "        Compute the eigenvalues and eigenvectors of the Laplacian matrix. The eigenvectors corresponding to the smallest eigenvalues encode the cluster structure in the data.\n",
    "\n",
    "4. Dimensionality Reduction:\n",
    "        Use the eigenvectors corresponding to the smallest eigenvalues as a reduced representation of the data.\n",
    "\n",
    "5. Clustering:\n",
    "        Apply a standard clustering algorithm (e.g., KMeans) to the reduced representation to obtain the final clusters.\n",
    "\n",
    "Spectral Clustering is powerful for identifying clusters with intricate shapes and is robust to noise and outliers.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for Spectral Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with non-convex clusters\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Apply Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=4, random_state=42, affinity='nearest_neighbors')\n",
    "labels = spectral.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with non-convex clusters using make_blobs from scikit-learn and then apply Spectral Clustering. The resulting clusters are visualized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2. Similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clustering graph and network data, similarity measures play a crucial role in quantifying the degree of resemblance or relatedness between nodes. These measures help in constructing affinity matrices, which are essential for various clustering algorithms. Some common similarity measures for graph and network data include:\n",
    "\n",
    "- Jaccard Similarity:\n",
    "        Measures the similarity between two sets by dividing the size of their intersection by the size of their union.\n",
    "\n",
    "- Cosine Similarity:\n",
    "        Measures the cosine of the angle between two vectors. In the context of graphs, vectors represent node neighborhoods or attribute vectors.\n",
    "\n",
    "- Graph Edit Distance:\n",
    "        Quantifies the dissimilarity between two graphs by measuring the minimum cost of transforming one graph into another through a series of edit operations (e.g., node deletions, insertions, or edge modifications).\n",
    "\n",
    "- Node and Edge Overlap:\n",
    "        Measures the overlap of nodes or edges between two graphs, providing a simple measure of similarity based on shared elements.\n",
    "\n",
    "- Pearson Correlation Coefficient:\n",
    "        Measures the linear correlation between two variables. In the context of graphs, it can be used to measure the correlation between node attributes.\n",
    "\n",
    "Selecting an appropriate similarity measure depends on the characteristics of the graph data and the specific clustering task at hand.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for calculating Jaccard Similarity on graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "# Create a graph (replace this with your graph data)\n",
    "G1 = nx.Graph([(1, 2), (2, 3), (3, 4)])\n",
    "G2 = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 5)])\n",
    "\n",
    "# Convert graph data to adjacency matrices\n",
    "adj_matrix1 = nx.to_numpy_array(G1)\n",
    "adj_matrix2 = nx.to_numpy_array(G2)\n",
    "\n",
    "# Flatten the adjacency matrices for Jaccard Similarity calculation\n",
    "flat_matrix1 = adj_matrix1.flatten()\n",
    "flat_matrix2 = adj_matrix2.flatten()\n",
    "\n",
    "# Calculate Jaccard Similarity\n",
    "jaccard_similarity = jaccard_similarity_score(flat_matrix1, flat_matrix2)\n",
    "\n",
    "print(f\"Jaccard Similarity between G1 and G2: {jaccard_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create two simple graphs using NetworkX, convert them to adjacency matrices, flatten the matrices, and then calculate the Jaccard Similarity between them using scikit-learn's jaccard_similarity_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3. Graph clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph clustering methods aim to identify groups of nodes within a graph that are more densely connected to each other than to nodes outside the group. Clustering in graphs is particularly useful for uncovering community structures, detecting functional modules, or identifying cohesive groups of related entities. Some common graph clustering methods include:\n",
    "\n",
    "1. Louvain Modularity Optimization:\n",
    "        Maximizes the modularity of a partition, which measures the density of edges within clusters compared to the density expected in a random network.\n",
    "\n",
    "2. Spectral Clustering:\n",
    "        Applies spectral decomposition to the Laplacian matrix of the graph to identify clusters based on the eigenvectors corresponding to the smallest eigenvalues.\n",
    "\n",
    "3. Walktrap:\n",
    "        Measures the similarity between nodes based on random walks in the graph, with nodes that are frequently visited together considered more similar.\n",
    "\n",
    "4. Infomap:\n",
    "        Models the network as a flow of information and seeks to find partitions that compress the description length of the information flow.\n",
    "\n",
    "5. Kernighan-Lin Bisection:\n",
    "        A heuristic method that recursively partitions the graph into two halves by optimizing a cost function related to edge cuts.\n",
    "\n",
    "These methods can be applied to various types of graphs, including social networks, biological networks, citation networks, and more.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for graph clustering using Louvain Modularity Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph (replace this with your graph data)\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Apply Louvain Modularity Optimization\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "# Visualize the results\n",
    "pos = nx.spring_layout(G)\n",
    "colors = [partition[node] for node in G.nodes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, pos, node_color=colors, with_labels=True, cmap=plt.cm.viridis)\n",
    "plt.title('Graph Clustering using Louvain Modularity Optimization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the Zachary's Karate Club graph as an example graph from NetworkX and apply Louvain Modularity Optimization for graph clustering. The resulting clusters are visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.1. Semisupervised clustering on partially labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised clustering deals with scenarios where only a subset of the data points in a dataset are labeled. This approach incorporates both labeled and unlabeled data to enhance the clustering process. In partially labeled data, only a fraction of the data points have ground truth labels, while the rest are unlabeled. Semi-supervised clustering methods aim to leverage the available labeled information to guide the clustering of the entire dataset.\n",
    "\n",
    "Key points about semi-supervised clustering on partially labeled data:\n",
    "\n",
    "- Incorporating Labeled Information:\n",
    "        Semi-supervised clustering integrates labeled instances into the clustering process, enhancing the accuracy of cluster assignments.\n",
    "\n",
    "- Soft Constraints:\n",
    "        Algorithms often use soft constraints that encourage similar behavior between labeled and unlabeled data points, allowing the model to learn from both sources.\n",
    "\n",
    "- Hybrid Approaches:\n",
    "        Semi-supervised clustering methods can combine traditional clustering techniques with supervised learning algorithms to create a hybrid model.\n",
    "\n",
    "- Handling Noisy Labels:\n",
    "        These methods are designed to handle noise and errors in the labeled data, providing robust clustering in the presence of imperfect labels.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering on partially labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Label a small fraction of the data (simulate partially labeled data)\n",
    "rng = np.random.RandomState(42)\n",
    "random_unlabeled_points = rng.rand(len(y)) < 0.9\n",
    "y[random_unlabeled_points] = -1  # Assign -1 to indicate unlabeled points\n",
    "\n",
    "# Apply KMeans to the entire dataset (including unlabeled points)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Apply LabelPropagation using the partially labeled data\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('KMeans Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters using make_blobs from scikit-learn. We label a small fraction of the data and use KMeans for clustering the entire dataset. Additionally, we apply LabelPropagation for semi-supervised clustering using the partially labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2. Semisupervised clustering on pairwise constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised clustering using pairwise constraints involves providing the algorithm with pairwise relationships or constraints between data points. These constraints can be in the form of must-link constraints (indicating that two points must belong to the same cluster) or cannot-link constraints (indicating that two points cannot belong to the same cluster). By incorporating such pairwise information, the clustering algorithm guides the grouping of data points, taking into account the specified relationships.\n",
    "\n",
    "Key points about semi-supervised clustering on pairwise constraints:\n",
    "\n",
    "- Must-Link and Cannot-Link Constraints:\n",
    "        Must-link constraints specify that two data points should belong to the same cluster, while cannot-link constraints specify that they should be in different clusters.\n",
    "\n",
    "- Constraint Incorporation:\n",
    "        Algorithms leverage these pairwise constraints to guide the clustering process, adjusting cluster assignments based on the specified relationships.\n",
    "\n",
    "- Handling Noisy Constraints:\n",
    "        Semi-supervised clustering methods on pairwise constraints are designed to handle noisy or conflicting constraints, providing robustness to imperfect information.\n",
    "\n",
    "- Improving Clustering Accuracy:\n",
    "        By incorporating pairwise constraints, the algorithm aims to improve the accuracy of cluster assignments, especially when dealing with complex or overlapping structures.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering on pairwise constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Introduce pairwise constraints (simulating noisy constraints)\n",
    "rng = np.random.RandomState(42)\n",
    "must_link = list(combinations(range(len(y)), 2))\n",
    "cannot_link = list(combinations(rng.choice(np.where(y == 0)[0], 2), 2))\n",
    "\n",
    "# Create a pairwise affinity matrix\n",
    "affinity_matrix = np.ones((len(y), len(y)))\n",
    "for i, j in must_link:\n",
    "    affinity_matrix[i, j] = 1\n",
    "    affinity_matrix[j, i] = 1\n",
    "\n",
    "for i, j in cannot_link:\n",
    "    affinity_matrix[i, j] = 0\n",
    "    affinity_matrix[j, i] = 0\n",
    "\n",
    "# Apply LabelPropagation using pairwise constraints\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y, affinity_matrix=affinity_matrix)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.title('True Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering with Pairwise Constraints')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters using make_blobs from scikit-learn. We introduce pairwise constraints (must-link and cannot-link) and apply LabelPropagation for semi-supervised clustering using these constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.3. Other types of background knowledge for semisupervised clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In semi-supervised clustering, leveraging additional background knowledge beyond pairwise constraints or labeled instances can further enhance the clustering process. Other types of background knowledge may include:\n",
    "\n",
    "- Attribute Constraints:\n",
    "        Constraints on the attributes or features of data points can guide the clustering algorithm. For example, specifying that certain features should be similar or dissimilar across clusters.\n",
    "\n",
    "- Hierarchy Information:\n",
    "        Information about hierarchical relationships between clusters or data points can be incorporated. This is particularly useful when the data exhibits a nested or hierarchical structure.\n",
    "\n",
    "- Prior Probabilities:\n",
    "        Providing prior probabilities for data points belonging to specific clusters can guide the algorithm in situations where certain clusters are expected to be more prevalent.\n",
    "\n",
    "- Spatial Constraints:\n",
    "        Incorporating constraints based on the spatial arrangement of data points can be useful, especially in applications where the proximity or arrangement holds meaningful information.\n",
    "\n",
    "- Temporal Information:\n",
    "        For time-series data, temporal constraints or information about the temporal ordering of data points can be valuable in guiding the clustering process.\n",
    "\n",
    "The incorporation of diverse types of background knowledge allows semi-supervised clustering algorithms to adapt to various data characteristics and application-specific requirements.\n",
    "\n",
    "Now, let's provide a real-world example of practical use in Python for semi-supervised clustering with attribute constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with three clusters and additional attribute information\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "attribute_information = np.random.rand(len(y), 5)  # Simulated additional attribute information\n",
    "\n",
    "# Introduce attribute constraints (simulating constraints on attributes)\n",
    "attribute_constraints = []\n",
    "for i in range(5):\n",
    "    cluster1, cluster2 = np.random.choice(range(3), size=2, replace=False)\n",
    "    attribute_constraints.append((cluster1, cluster2, i))  # Attribute i should be similar/dissimilar in clusters\n",
    "\n",
    "# Apply Spectral Clustering without using attribute constraints\n",
    "spectral = SpectralClustering(n_clusters=3, random_state=42)\n",
    "spectral_labels = spectral.fit_predict(X)\n",
    "\n",
    "# Apply LabelPropagation using attribute constraints\n",
    "label_propagation = LabelPropagation(kernel='knn', n_neighbors=10)\n",
    "label_propagation.fit(X, y, node_features=attribute_information, attribute_constraints=attribute_constraints)\n",
    "lp_labels = label_propagation.predict(X)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=spectral_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('Spectral Clustering without Attribute Constraints')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=lp_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('LabelPropagation Clustering with Attribute Constraints')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we generate synthetic data with three clusters and additional attribute information. We introduce attribute constraints and compare the results of Spectral Clustering without using attribute constraints and LabelPropagation with attribute constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
