{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Refer to E-Book 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1.1. Mining multilevel associations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining multilevel associations involves discovering patterns and associations at different levels of granularity within a dataset. This technique allows for the exploration of relationships and dependencies not only at the individual item level but also at higher levels of abstraction, such as categories, hierarchies, or combinations of attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Multilevel Association Rules:\n",
        "    Multilevel association rules extend the concept of association rules to multiple levels of abstraction.\n",
        "    These rules can reveal relationships between items, categories, or attributes, providing a more comprehensive understanding of the data.\n",
        "\n",
        "2. Hierarchical Structures:\n",
        "    Mining multilevel associations often involves leveraging hierarchical structures or taxonomies within the dataset.\n",
        "    For example, in a retail dataset, items might be organized into categories, and associations can be explored at both the item and category levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining Multilevel Associations in Python:\n",
        "\n",
        "Let's use a retail dataset with hierarchical structures to demonstrate the mining of multilevel associations in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the mlxtend library if not already installed\n",
        "# pip install mlxtend\n",
        "\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a retail dataset with categories\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['Apple', 'Banana', 'Apple', 'Banana', 'Apple', 'Banana', 'Orange', 'Apple', 'Banana', 'Orange'],\n",
        "    'Category': ['Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit', 'Fruit']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Display the rules with item and category levels\n",
        "print(\"Multilevel Association Rules:\")\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the retail dataset includes both individual items (e.g., Apple, Banana) and their corresponding categories (e.g., Fruit). The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. The rules provide insights into associations at both the item and category levels, illustrating the mining of multilevel associations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1.2. Mining multidimensional associations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining multidimensional associations involves exploring patterns and relationships across multiple dimensions or attributes in a dataset. Instead of focusing on individual items or attributes, this technique aims to discover associations that span multiple dimensions, providing a more comprehensive understanding of the interdependencies within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Multidimensional Association Rules:\n",
        "    Multidimensional association rules consider associations involving multiple attributes or dimensions simultaneously.\n",
        "    These rules capture complex relationships that cannot be adequately represented by examining individual attributes in isolation.\n",
        "\n",
        "2. Data Cube Representation:\n",
        "    Mining multidimensional associations is often associated with the use of a data cube, where each cell in the cube represents a combination of attribute values, and associations are analyzed across these combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining Multidimensional Associations in Python:\n",
        "\n",
        "Let's use a dataset with multiple dimensions to demonstrate the mining of multidimensional associations in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the mlxtend library if not already installed\n",
        "# pip install mlxtend\n",
        "\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with multiple dimensions\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['Apple', 'Banana', 'Apple', 'Banana', 'Apple', 'Banana', 'Orange', 'Apple', 'Banana', 'Orange'],\n",
        "    'Location': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=[df['Item'], df['Location']]).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Display the rules with multiple dimensions\n",
        "print(\"Multidimensional Association Rules:\")\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the dataset includes dimensions such as 'Item' and 'Location.' The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. These rules capture associations across multiple dimensions, illustrating the mining of multidimensional associations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1.3. Mining quantitative association rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining quantitative association rules involves discovering patterns and relationships that involve numeric or quantitative attributes in a dataset. Unlike traditional association rule mining that primarily deals with categorical data, this technique extends the analysis to include the quantitative aspects of the data, allowing for the identification of associations among numeric values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Numeric Attributes:\n",
        "    Mining quantitative association rules focuses on exploring associations between numeric attributes in the dataset.\n",
        "    Numeric attributes may include features like age, income, temperature, or any other measurable quantity.\n",
        "\n",
        "2. Quantitative Measures:\n",
        "    Quantitative association rules are evaluated using metrics appropriate for numeric data, such as correlation coefficients, regression analysis, or other statistical measures.\n",
        "\n",
        "3. Real-World Relevance:\n",
        "    Quantitative association rules can provide insights into the relationships and dependencies among numeric variables, leading to more informed decision-making in various domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining Quantitative Association Rules in Python:\n",
        "\n",
        "Let's use a dataset with numeric attributes to demonstrate the mining of quantitative association rules in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with numeric attributes\n",
        "data = {\n",
        "    'TransactionID': [1, 2, 3, 4, 5],\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Income': [50000, 60000, 75000, 80000, 90000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Age']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Display the rules with numeric attributes\n",
        "print(\"Quantitative Association Rules:\")\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the dataset includes a numeric attribute 'Age.' The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. These rules capture associations involving the numeric attribute, illustrating the mining of quantitative association rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1.4. Mining high-dimensional data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining high-dimensional data involves exploring patterns and relationships in datasets with a large number of dimensions or attributes. In high-dimensional spaces, traditional data mining techniques may face challenges such as the curse of dimensionality. Therefore, specialized methods are employed to extract meaningful insights from these complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Curse of Dimensionality:\n",
        "    The curse of dimensionality refers to the challenges and increased complexity that arise as the number of dimensions in a dataset grows.\n",
        "    High-dimensional data can lead to sparsity, increased computational requirements, and difficulties in visualizing and interpreting results.\n",
        "\n",
        "2. Dimensionality Reduction:\n",
        "    Dimensionality reduction techniques are often applied to high-dimensional data to simplify the dataset while preserving important patterns and relationships.\n",
        "    Methods like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) are commonly used.\n",
        "\n",
        "3. Specialized Algorithms:\n",
        "    Specialized algorithms designed for high-dimensional data, such as frequent pattern mining algorithms or clustering techniques, help reveal patterns that might be hidden in the complexity of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining High-Dimensional Data in Python:\n",
        "\n",
        "Let's use a high-dimensional dataset and apply dimensionality reduction techniques to demonstrate mining high-dimensional data in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic high-dimensional dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n",
        "\n",
        "# Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Visualize the reduced-dimensional data\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\n",
        "plt.title('PCA: Dimensionality Reduction')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, we generate a synthetic high-dimensional dataset using the make_classification function from scikit-learn. We then apply PCA to reduce the dataset to two dimensions for visualization purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1.5. Mining rare patterns and negative patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining rare patterns and negative patterns involves the identification of infrequent or uncommon patterns in a dataset. While traditional data mining often focuses on finding frequent patterns, the analysis of rare and negative patterns is valuable for understanding outliers, anomalies, and relationships that may not be immediately apparent. These patterns can offer insights into unusual occurrences or deviations from the norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Rare Patterns:\n",
        "    Rare patterns refer to infrequent or uncommon occurrences in the data.\n",
        "    These patterns may represent events that happen rarely but can be significant, such as rare diseases, unusual customer behaviors, or outliers.\n",
        "\n",
        "2. Negative Patterns:\n",
        "    Negative patterns involve the absence or lack of certain associations in the data.\n",
        "    Discovering negative patterns is useful for understanding relationships where the absence of a particular attribute or event is noteworthy.\n",
        "\n",
        "3. Applications:\n",
        "    Mining rare and negative patterns is applicable in various domains, including fraud detection, anomaly detection, healthcare, and quality control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining Rare Patterns and Negative Patterns in Python:\n",
        "\n",
        "Let's use a dataset with rare patterns and apply data mining techniques to identify both rare and negative patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with rare patterns\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Display the rare and negative patterns\n",
        "print(\"Association Rules with Rare and Negative Patterns:\")\n",
        "print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, we use a simplified dataset representing transactions with items. The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. These rules capture rare patterns, providing insights into infrequent associations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2.1. Mining compressed patterns by pattern clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining compressed patterns involves the identification and extraction of compact and meaningful representations of patterns within a dataset. Pattern clustering is a technique used to group similar patterns together, creating a more condensed and interpretable view of the data. This approach is valuable for simplifying complex datasets and revealing underlying structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Pattern Clustering:\n",
        "    Pattern clustering involves grouping similar patterns based on certain criteria or similarity measures.\n",
        "    Clustering techniques, such as K-means or hierarchical clustering, can be applied to identify clusters of related patterns.\n",
        "\n",
        "2. Compressed Patterns:\n",
        "    Compressed patterns are compact representations of clusters, providing a summary of the underlying patterns within the data.\n",
        "    These compressed patterns can simplify the analysis and enhance interpretability.\n",
        "\n",
        "3. Applications:\n",
        "    Mining compressed patterns is applicable in various domains, including customer segmentation, anomaly detection, and summarization of large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example of Mining Compressed Patterns by Pattern Clustering in Python:\n",
        "\n",
        "Let's use a dataset and apply K-means clustering to demonstrate mining compressed patterns in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic dataset with patterns\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3, 8, 9, 10, 15, 16, 17],\n",
        "    'Feature2': [5, 6, 7, 12, 13, 14, 19, 20, 21]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply K-means clustering to identify compressed patterns\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(df)\n",
        "\n",
        "# Visualize the compressed patterns\n",
        "plt.scatter(df['Feature1'], df['Feature2'], c=df['Cluster'], cmap='viridis', edgecolor='k', s=40)\n",
        "plt.title('Pattern Clustering: Compressed Patterns')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, we generate a synthetic dataset with patterns in two features. K-means clustering is applied to identify three clusters, grouping similar patterns together. The resulting compressed patterns are visualized for better interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2.2. Extracting redundancy-aware top-k patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting redundancy-aware top-k patterns involves identifying and ranking patterns in a dataset based on their significance while considering redundancy. Redundancy-aware mining aims to provide a concise set of patterns that capture diverse and non-repetitive information, avoiding the inclusion of similar or redundant patterns in the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Concepts:\n",
        "\n",
        "1. Redundancy-Aware Mining:\n",
        "    Redundancy-aware mining techniques take into account the presence of redundant patterns in the dataset.\n",
        "    Redundancy can arise when patterns share similar information, and extracting diverse, non-repetitive patterns is essential for meaningful results.\n",
        "\n",
        "2. Top-k Patterns:\n",
        "    The concept of top-k patterns involves ranking patterns based on a certain criterion (e.g., support, confidence) and extracting the top-k most significant patterns.\n",
        "    This approach allows analysts to focus on a limited number of highly relevant patterns.\n",
        "\n",
        "3. Applications:\n",
        "    Extracting redundancy-aware top-k patterns is applicable in various domains, including marketing, healthcare, and fraud detection, where concise and non-repetitive insights are crucial.\n",
        "\n",
        "#### Example of Extracting Redundancy-Aware Top-k Patterns in Python:\n",
        "\n",
        "Let's use a dataset and apply redundancy-aware top-k pattern mining to extract significant and non-redundant patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with patterns\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm to find frequent itemsets\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules with redundancy-aware top-k extraction\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "top_k_rules = rules.sort_values(by='confidence', ascending=False).head(2)  # Extract top 2 rules\n",
        "\n",
        "# Display the redundancy-aware top-k patterns\n",
        "print(\"Redundancy-Aware Top-k Patterns:\")\n",
        "print(top_k_rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the dataset represents transactions with items. The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. Redundancy-aware top-k patterns are then extracted by sorting the rules based on confidence and selecting the top-k most significant rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.1. Pruning pattern space with pattern pruning constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pruning pattern space with pattern pruning constraints involves the application of constraints to reduce the search space and focus on extracting patterns that satisfy specific criteria. This technique is employed to streamline the pattern mining process, making it more efficient and targeted by eliminating patterns that do not align with predefined constraints.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Pattern Pruning Constraints:\n",
        "    Pattern pruning constraints are rules or conditions applied during the pattern mining process to eliminate patterns that do not meet certain criteria.\n",
        "    These constraints help in focusing the analysis on patterns that are more relevant or meaningful based on specific requirements.\n",
        "\n",
        "2. Efficiency Improvement:\n",
        "    Pruning pattern space enhances the efficiency of pattern mining algorithms by reducing the computational burden associated with exploring the entire search space.\n",
        "    Constraints eliminate patterns early in the process, allowing the algorithm to concentrate on potentially more interesting patterns.\n",
        "\n",
        "3. Applications:\n",
        "    Pruning pattern space is applicable in various scenarios, such as market basket analysis, where constraints may include minimum support or confidence thresholds, or in bioinformatics, where constraints could involve sequence length or similarity criteria.\n",
        "\n",
        "#### Example of Pruning Pattern Space with Pattern Pruning Constraints in Python:\n",
        "\n",
        "Let's use a dataset and apply pattern pruning constraints to streamline the extraction of meaningful patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with patterns\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm with pattern pruning constraints\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Apply pattern pruning constraints (e.g., minimum lift)\n",
        "pruned_rules = rules[rules['lift'] > 1.2]\n",
        "\n",
        "# Display the pruned patterns\n",
        "print(\"Pruned Patterns:\")\n",
        "print(pruned_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, a dataset representing transactions with items is used. The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. Pattern pruning constraints, in this case, involve selecting rules with a minimum lift value, streamlining the patterns to those with higher lift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.2. Pruning data space with data pruning constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pruning data space with data pruning constraints involves the application of specific criteria to filter and reduce the dataset before initiating the data mining process. By selectively removing irrelevant or less informative data, this technique aims to enhance the efficiency and effectiveness of data mining algorithms, allowing them to focus on the most pertinent information.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Data Pruning Constraints:\n",
        "    Data pruning constraints are conditions or rules applied to the dataset to selectively remove certain data points or features.\n",
        "    These constraints are designed to eliminate less relevant or noisy data, enhancing the quality of the dataset for subsequent mining.\n",
        "\n",
        "2. Efficiency Improvement:\n",
        "    Pruning data space improves the efficiency of data mining algorithms by reducing the volume of data that needs to be processed.\n",
        "    Removing irrelevant or redundant data early in the process streamlines subsequent analyses, leading to faster and more focused results.\n",
        "\n",
        "3. Applications:\n",
        "    Data pruning is applicable in various scenarios, such as preprocessing for machine learning, where irrelevant features or instances are removed, or in text mining, where stop-word removal is a form of data pruning.\n",
        "\n",
        "#### Example of Pruning Data Space with Data Pruning Constraints in Python:\n",
        "\n",
        "Let's use a dataset and apply data pruning constraints to preprocess the data before data mining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create a DataFrame for demonstration purposes\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Apply data pruning constraints (e.g., removing a specific class)\n",
        "pruned_df = df[df['target'] != 1]\n",
        "\n",
        "# Display the pruned dataset\n",
        "print(\"Pruned Dataset:\")\n",
        "print(pruned_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the Iris dataset is loaded, and a DataFrame is created. Data pruning constraints involve removing instances associated with a specific class (e.g., class 1). This pruned dataset can then be used for subsequent data mining or analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.3. Mining space pruning with succinctness constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mining space pruning with succinctness constraints involves the application of constraints to limit the size or complexity of the pattern space explored during data mining. The goal is to extract concise and meaningful patterns by controlling the search space, thereby improving the efficiency and interpretability of the mining process.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Space Pruning Constraints:\n",
        "    Space pruning constraints are rules or criteria applied during the mining process to limit the size or complexity of the pattern space.\n",
        "    These constraints help focus on a more manageable set of patterns, enhancing interpretability and reducing computational requirements.\n",
        "\n",
        "2. Succinctness:\n",
        "    Succinctness refers to the brevity or compactness of patterns. Mining with succinctness constraints aims to extract concise and meaningful patterns that capture essential information from the data.\n",
        "\n",
        "3. Efficiency and Interpretability:\n",
        "    By pruning the pattern space with succinctness constraints, the mining process becomes more efficient, as it concentrates on relevant patterns.\n",
        "    Succinct patterns are often easier to interpret and communicate, making them more valuable in decision-making processes.\n",
        "\n",
        "4. Applications:\n",
        "    Mining space pruning with succinctness constraints is applicable in various domains, such as market basket analysis, where concise association rules are preferred, or in text mining, where succinct patterns may represent key topics or themes.\n",
        "\n",
        "#### Example of Mining Space Pruning with Succinctness Constraints in Python:\n",
        "\n",
        "Let's use a dataset and apply succinctness constraints to limit the size of the pattern space during association rule mining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Simulate a dataset with patterns\n",
        "data = {\n",
        "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
        "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Apply Apriori algorithm with succinctness constraints\n",
        "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
        "\n",
        "# Apply succinctness constraints (e.g., maximum antecedent length)\n",
        "succinct_rules = rules[rules['antecedents'].apply(len) <= 2]\n",
        "\n",
        "# Display the succinct patterns\n",
        "print(\"Succinct Association Rules:\")\n",
        "print(succinct_rules[['antecedents', 'consequents', 'support', 'confidence']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, a dataset represents transactions with items. The Apriori algorithm is applied to discover frequent itemsets, and association rules are generated. Succinctness constraints involve selecting rules with a maximum antecedent length, limiting the complexity of the patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4.1. Sequential pattern mining: concepts and primitives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sequential pattern mining involves discovering patterns that capture the temporal order of events or items in a sequence. This technique is crucial for revealing hidden structures in sequential data, making it applicable in various domains such as time-series analysis, web clickstreams, and customer behavior tracking.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Sequential Patterns:\n",
        "    Sequential patterns represent the order in which events or items occur over time.\n",
        "    These patterns capture dependencies and temporal relationships within a sequence of data.\n",
        "\n",
        "2. Support:\n",
        "    Support in sequential pattern mining refers to the frequency with which a particular sequential pattern occurs in the dataset.\n",
        "    High support indicates that the pattern is common or frequent.\n",
        "\n",
        "3. Primitives:\n",
        "    Primitives in sequential pattern mining include elements such as events, items, and time intervals.\n",
        "    Basic operations involve defining sequences, specifying temporal constraints, and identifying frequent subsequences.\n",
        "\n",
        "4. Applications:\n",
        "    Sequential pattern mining is used in diverse applications, including analyzing user behavior on websites, predicting stock market trends, and understanding the progression of disease symptoms over time.\n",
        "\n",
        "#### Example of Sequential Pattern Mining in Python:\n",
        "\n",
        "Let's use a dataset of sequences and apply sequential pattern mining to discover frequent sequential patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prefixspan import PrefixSpan\n",
        "\n",
        "# Simulate a dataset with sequences\n",
        "sequences = [\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 5],\n",
        "    [1, 3, 4],\n",
        "    [2, 5],\n",
        "    [1, 2, 3]\n",
        "]\n",
        "\n",
        "# Apply PrefixSpan algorithm for sequential pattern mining\n",
        "min_support = 2\n",
        "patterns = PrefixSpan(sequences).frequent(min_support)\n",
        "\n",
        "# Display the discovered sequential patterns\n",
        "print(\"Frequent Sequential Patterns:\")\n",
        "for pattern in patterns:\n",
        "    print(pattern)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, a dataset consists of sequences (e.g., sequences of events, items, or actions). The PrefixSpan algorithm is applied to find frequent sequential patterns with a minimum support of 2. The output includes sequences that occur frequently in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4.2. Scalable methods for mining sequential patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scalable methods for mining sequential patterns address the challenge of efficiently extracting patterns from large-scale datasets. As datasets grow in size, traditional sequential pattern mining algorithms may become computationally expensive. Scalable methods aim to enhance the efficiency of the mining process, making it feasible to analyze massive amounts of sequential data.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Vertical Data Representation:\n",
        "    Vertical data representation is a technique where data is organized vertically, facilitating the identification of frequent items and their positions in sequences.\n",
        "    This representation allows for more efficient mining, especially when dealing with large datasets.\n",
        "\n",
        "2. Projection-Based Techniques:\n",
        "    Projection-based techniques involve creating projections of the original sequence database based on certain conditions.\n",
        "    These techniques reduce the search space by focusing on relevant subsequences, improving the scalability of the mining process.\n",
        "\n",
        "3. Parallelization:\n",
        "    Parallelization involves distributing the mining task across multiple computing resources, enabling simultaneous processing and faster pattern discovery.\n",
        "    This approach is particularly effective for large-scale datasets.\n",
        "\n",
        "4. Applications:\n",
        "    Scalable methods for mining sequential patterns find applications in diverse fields, including analyzing massive web clickstream data, identifying patterns in extensive healthcare records, and understanding user behavior in large-scale e-commerce platforms.\n",
        "\n",
        "#### Example of Scalable Sequential Pattern Mining in Python:\n",
        "\n",
        "Let's use a large-scale dataset and apply a scalable sequential pattern mining algorithm to efficiently discover frequent sequential patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prefixspan import BIDE\n",
        "\n",
        "# Simulate a large-scale dataset with sequences\n",
        "large_sequences = [\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [2, 3, 5],\n",
        "    [1, 3, 4],\n",
        "    [2, 5],\n",
        "    [1, 2, 3, 6, 7, 8, 9, 10],\n",
        "    # ... (additional large-scale sequences)\n",
        "]\n",
        "\n",
        "# Apply BIDE (Bidirectional Expansion) algorithm for scalable sequential pattern mining\n",
        "min_support = 2\n",
        "patterns = BIDE(large_sequences).frequent(min_support)\n",
        "\n",
        "# Display the discovered frequent sequential patterns\n",
        "print(\"Frequent Sequential Patterns (Scalable):\")\n",
        "for pattern in patterns:\n",
        "    print(pattern)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the BIDE algorithm, known for its scalability, is applied to a large-scale dataset. The algorithm efficiently discovers frequent sequential patterns with a minimum support of 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4.3. Constraint-based mining of sequential patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constraint-based mining of sequential patterns involves incorporating specific conditions or constraints during the pattern mining process. These constraints guide the algorithm to discover patterns that satisfy predefined criteria, making the mining process more targeted and aligned with the user's domain knowledge or analytical goals.\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. Constraints:\n",
        "    Constraints are conditions or rules applied to the mining process, restricting the patterns that are considered during analysis.\n",
        "    These constraints can be based on various factors, such as item attributes, temporal relationships, or specific sequence characteristics.\n",
        "\n",
        "2. Customization:\n",
        "    Constraint-based mining allows users to customize the mining process according to their specific requirements.\n",
        "    Users can define constraints to filter out patterns that do not meet certain criteria, resulting in more meaningful and relevant discoveries.\n",
        "\n",
        "3. Interpretability:\n",
        "    The use of constraints enhances the interpretability of the discovered patterns.\n",
        "    By incorporating domain-specific knowledge into the mining process, the results are more aligned with the user's understanding and can be more easily interpreted.\n",
        "\n",
        "4. Applications:\n",
        "    Constraint-based mining of sequential patterns finds applications in diverse domains, including healthcare for identifying patient-specific treatment patterns, in manufacturing for optimizing production sequences, and in finance for detecting fraud patterns with specific characteristics.\n",
        "\n",
        "#### Example of Constraint-Based Sequential Pattern Mining in Python:\n",
        "\n",
        "Let's use a dataset and apply constraint-based sequential pattern mining to discover patterns based on specific conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prefixspan import PrefixSpan\n",
        "\n",
        "# Simulate a dataset with sequences\n",
        "sequences = [\n",
        "    [1, 2, 3, 4],\n",
        "    [2, 3, 5],\n",
        "    [1, 3, 4],\n",
        "    [2, 5],\n",
        "    [1, 2, 3]\n",
        "]\n",
        "\n",
        "# Apply PrefixSpan algorithm for sequential pattern mining with constraints\n",
        "min_support = 2\n",
        "patterns = PrefixSpan(sequences).frequent(min_support)\n",
        "\n",
        "# Apply constraints (e.g., patterns with a specific item)\n",
        "constraint_item = 3\n",
        "constrained_patterns = [pattern for pattern in patterns if constraint_item in pattern]\n",
        "\n",
        "# Display the constrained sequential patterns\n",
        "print(f\"Constrained Sequential Patterns (with item {constraint_item}):\")\n",
        "for pattern in constrained_patterns:\n",
        "    print(pattern)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the PrefixSpan algorithm is applied to a dataset of sequences. The constraint involves selecting patterns that contain a specific item (e.g., item 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5.1. Methods for mining frequent subgraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph dataset (represented by networkx):\n",
            "Graph 1: Nodes=[1, 2, 3, 4], Edges=[(1, 2), (2, 3), (3, 4)]\n",
            "Graph 2: Nodes=[1, 2, 3, 4, 5], Edges=[(1, 2), (2, 3), (3, 4), (4, 5)]\n",
            "Graph 3: Nodes=[1, 2, 3, 4, 6], Edges=[(1, 2), (2, 3), (3, 4), (4, 6)]\n",
            "\n",
            "Considering edges as basic subgraphs for this example (min_support=2):\n",
            "  Frequent Edge: (1, 2), Support: 3\n",
            "  Frequent Edge: (2, 3), Support: 3\n",
            "  Frequent Edge: (3, 4), Support: 3\n",
            "\n",
            "Discovered Frequent Subgraphs (conceptual example): \n",
            "  Subgraph: (1, 2), Support: 3\n",
            "  Subgraph: (2, 3), Support: 3\n",
            "  Subgraph: (3, 4), Support: 3\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "graphs = [\n",
        "    nx.Graph([(1, 2), (2, 3), (3, 4)]),\n",
        "    nx.Graph([(1, 2), (2, 3), (3, 4), (4, 5)]),\n",
        "    nx.Graph([(1, 2), (2, 3), (3, 4), (4, 6)]),\n",
        "]\n",
        "\n",
        "print(\"Graph dataset (represented by networkx):\")\n",
        "for i, graph in enumerate(graphs):\n",
        "    print(f\"Graph {i+1}: Nodes={list(graph.nodes)}, Edges={list(graph.edges)}\")\n",
        "\n",
        "def find_frequent_subgraphs_bruteforce(graphs, min_support):\n",
        "    edge_counts = {}\n",
        "    for graph in graphs:\n",
        "        for u, v in graph.edges():\n",
        "            edge = tuple(sorted((u, v)))\n",
        "            edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "            \n",
        "    frequent_subgraphs = []\n",
        "    print(f\"\\nConsidering edges as basic subgraphs for this example (min_support={min_support}):\")\n",
        "    for edge, count in edge_counts.items():\n",
        "        if count >= min_support:\n",
        "            frequent_subgraphs.append((edge, count))\n",
        "            print(f\"  Frequent Edge: {edge}, Support: {count}\")\n",
        "            \n",
        "    return frequent_subgraphs\n",
        "\n",
        "min_support = 2\n",
        "frequent_subgraphs = find_frequent_subgraphs_bruteforce(graphs, min_support)\n",
        "\n",
        "print(\"\\nDiscovered Frequent Subgraphs (conceptual example): \")\n",
        "if frequent_subgraphs:\n",
        "    for subgraph, support in frequent_subgraphs:\n",
        "        print(f\"  Subgraph: {subgraph}, Support: {support}\")\n",
        "else:\n",
        "    print(\"  No frequent subgraphs found with the given minimum support.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the gSpan algorithm is applied to a dataset of graphs. The algorithm identifies frequent subgraphs with a minimum support of 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5.2. Mining variant and constrained substructure patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph dataset with attributes (represented by networkx):\n",
            "Graph 1: Nodes=[(1, {'attribute': 'A'}), (2, {'attribute': 'B'})], Edges=[(1, 2)]\n",
            "Graph 2: Nodes=[(1, {'attribute': 'A'}), (2, {'attribute': 'C'})], Edges=[(1, 2)]\n",
            "Graph 3: Nodes=[(1, {'attribute': 'B'}), (2, {'attribute': 'B'})], Edges=[(1, 2)]\n",
            "\n",
            "Considering nodes with attribute 'B' as basic constrained subgraphs (min_support=1):\n",
            "  Constrained Node: 2, Support: 2\n",
            "  Constrained Node: 1, Support: 1\n",
            "\n",
            "Discovered Constrained Substructure Patterns (with attribute 'B', conceptual example): \n",
            "  Subgraph (Node): 2, Support: 2\n",
            "  Subgraph (Node): 1, Support: 1\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "graphs_with_attributes = [\n",
        "    nx.Graph(),\n",
        "    nx.Graph(),\n",
        "    nx.Graph(),\n",
        "]\n",
        "\n",
        "graphs_with_attributes[0].add_nodes_from([(1, {'attribute': 'A'}), (2, {'attribute': 'B'})])\n",
        "graphs_with_attributes[0].add_edge(1, 2)\n",
        "\n",
        "graphs_with_attributes[1].add_nodes_from([(1, {'attribute': 'A'}), (2, {'attribute': 'C'})])\n",
        "graphs_with_attributes[1].add_edge(1, 2)\n",
        "\n",
        "graphs_with_attributes[2].add_nodes_from([(1, {'attribute': 'B'}), (2, {'attribute': 'B'})])\n",
        "graphs_with_attributes[2].add_edge(1, 2)\n",
        "\n",
        "print(\"Graph dataset with attributes (represented by networkx):\")\n",
        "for i, graph in enumerate(graphs_with_attributes):\n",
        "    print(f\"Graph {i+1}: Nodes={list(graph.nodes(data=True))}, Edges={list(graph.edges)}\")\n",
        "\n",
        "constraint_attribute = 'B'\n",
        "\n",
        "def find_constrained_subgraphs_bruteforce(graphs_with_attributes, constraint_attribute, min_support):\n",
        "    constrained_node_counts = {}\n",
        "    for graph in graphs_with_attributes:\n",
        "        for node, data in graph.nodes(data=True):\n",
        "            if 'attribute' in data and data['attribute'] == constraint_attribute:\n",
        "                constrained_node_counts[node] = constrained_node_counts.get(node, 0) + 1\n",
        "                \n",
        "    constrained_subgraphs = []\n",
        "    print(f\"\\nConsidering nodes with attribute '{constraint_attribute}' as basic constrained subgraphs (min_support={min_support}):\")\n",
        "    for node, count in constrained_node_counts.items():\n",
        "        if count >= min_support:\n",
        "            constrained_subgraphs.append((node, count))\n",
        "            print(f\"  Constrained Node: {node}, Support: {count}\")\n",
        "            \n",
        "    return constrained_subgraphs\n",
        "\n",
        "min_support_constrained = 1\n",
        "constrained_substructure_patterns = find_constrained_subgraphs_bruteforce(graphs_with_attributes, constraint_attribute, min_support_constrained)\n",
        "\n",
        "print(f\"\\nDiscovered Constrained Substructure Patterns (with attribute '{constraint_attribute}', conceptual example): \")\n",
        "if constrained_substructure_patterns:\n",
        "    for subgraph, support in constrained_substructure_patterns:\n",
        "        print(f\"  Subgraph (Node): {subgraph}, Support: {support}\")\n",
        "else:\n",
        "    print(\"  No constrained substructure patterns found with the given minimum support and constraint.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    In this example, the gSpan algorithm is applied to a dataset of graphs with node attributes. The mining process is constrained to patterns with a specific node attribute ('B')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6.1. Phrase mining in massive text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovered Multi-Word Phrases:\n",
            "data mining is an essential technique\n",
            "phrase mining enhances text analysis\n",
            "massive text data contains valuable insights\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
        "\n",
        "# Simulate a text corpus\n",
        "text_corpus = [\n",
        "    \"data mining is an essential technique\",\n",
        "    \"phrase mining enhances text analysis\",\n",
        "    \"massive text data contains valuable insights\",\n",
        "    # ... (additional text documents)\n",
        "]\n",
        "\n",
        "# Preprocess the text corpus and identify multi-word phrases\n",
        "phrases_model = Phrases(text_corpus, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
        "phraser = Phrases(text_corpus, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
        "phrases = list(phraser[text_corpus])\n",
        "\n",
        "# Display the discovered multi-word phrases\n",
        "print(\"Discovered Multi-Word Phrases:\")\n",
        "for document_phrases in phrases:\n",
        "    print(document_phrases)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
