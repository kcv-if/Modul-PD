{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refer to E-Book 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Market basket analysis: a motivating example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market Basket Analysis is a data mining technique used to discover associations and relationships between items purchased together. The primary goal is to identify patterns of co-occurrence among products in transactions, revealing insights into customer behavior and preferences. This analysis is widely used in retail, e-commerce, and other industries to optimize product placement, promotions, and cross-selling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Association Rules:\n",
    "    Market Basket Analysis is often associated with deriving rules of the form \"If a customer buys item X, they are likely to buy item Y as well.\"\n",
    "\n",
    "2. Support, Confidence, and Lift:\n",
    "    Support measures how frequently a set of items appears together, confidence measures the likelihood that item Y is bought when item X is bought, and lift indicates the strength of the association between items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Market Basket Analysis in Python:\n",
    "\n",
    "Let's use a Python example with the mlxtend library to perform Market Basket Analysis on a sample retail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for market basket analysis\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)\n",
    "\n",
    "# Display the frequent itemsets and association rules\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we simulate a retail dataset with transactions and items. The mlxtend library is then used to perform Apriori algorithm-based Market Basket Analysis. The output includes frequent itemsets and association rules, providing insights into which items are frequently purchased together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Frequent itemsets, closed itemsets, and association rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Market Basket Analysis, the concepts of frequent itemsets, closed itemsets, and association rules play a crucial role in identifying patterns and relationships within transactional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Frequent Itemsets:\n",
    "    A frequent itemset is a set of items that frequently appear together in transactions. It is determined based on a minimum support threshold, which specifies the minimum frequency or percentage of transactions in which the itemset should occur.\n",
    "\n",
    "2. Closed Itemsets:\n",
    "    A closed itemset is a frequent itemset for which none of its supersets have the same support. In other words, it is an itemset that is not a subset of any other frequent itemset with the same support. Closed itemsets provide a more compact representation of frequent patterns.\n",
    "\n",
    "3. Association Rules:\n",
    "    Association rules express relationships between items in the form of \"if item X is present, then item Y is likely to be present as well.\" These rules are derived from frequent itemsets and are characterized by metrics such as confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Frequent Itemsets, Closed Itemsets, and Association Rules in Python:\n",
    "\n",
    "Let's use the mlxtend library to demonstrate these concepts on a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for frequent itemset mining\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Find closed itemsets\n",
    "closed_itemsets = frequent_itemsets[frequent_itemsets['support'] == frequent_itemsets['support']]\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
    "\n",
    "# Display the frequent itemsets, closed itemsets, and association rules\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nClosed Itemsets:\")\n",
    "print(closed_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we use the mlxtend library to find frequent itemsets, closed itemsets, and association rules from a simulated retail dataset. The output includes sets of items that frequently occur together (frequent itemsets), closed itemsets that are not part of larger frequent itemsets with the same support, and association rules indicating relationships between items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Apriori Algorithm: Finding Frequent Itemsets by Confined Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a classic data mining algorithm used for discovering frequent itemsets in transactional datasets. It employs a level-wise approach to explore combinations of items and efficiently identifies those that meet a specified support threshold. The algorithm uses the Apriori property, which states that if an itemset is frequent, all of its subsets must also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Steps of the Apriori Algorithm:\n",
    "\n",
    "1. Candidate Generation:\n",
    "    The algorithm starts with individual items as candidates and iteratively generates larger candidate itemsets based on the frequent itemsets discovered in the previous iteration.\n",
    "\n",
    "2. Support Counting:\n",
    "    The support of each candidate itemset is calculated by counting the number of transactions containing the itemset.\n",
    "\n",
    "3. Pruning:\n",
    "    Itemsets with support below a predefined threshold are pruned, as they cannot be part of frequent itemsets.\n",
    "\n",
    "4. Iteration:\n",
    "    The process iterates until no new frequent itemsets can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Apriori Algorithm in Python:\n",
    "\n",
    "Let's use the mlxtend library to implement the Apriori algorithm on a sample retail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the Apriori algorithm is applied to find frequent itemsets in a retail dataset. The mlxtend library's apriori function is used to generate itemsets based on a minimum support threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Generating association rules from frequent itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once frequent itemsets have been identified using algorithms like Apriori, the next step in Market Basket Analysis is to generate association rules. Association rules reveal interesting relationships between items in the dataset, providing valuable insights for businesses to make informed decisions about product placement, marketing strategies, and cross-selling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Support, Confidence, and Lift:\n",
    "    These metrics are used to evaluate the strength and significance of association rules.\n",
    "    \n",
    "        Support: Measures the frequency of occurrence of an itemset.\n",
    "\n",
    "        Confidence: Measures the likelihood that item Y is bought when item X is bought.\n",
    "        \n",
    "        Lift: Indicates the strength of the association between items.\n",
    "\n",
    "2. Association Rule Format:\n",
    "\n",
    "        Association rules are typically represented as \"if X, then Y,\" where X and Y are itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Generating Association Rules in Python:\n",
    "\n",
    "Let's continue with the previous example and use the mlxtend library to generate association rules from the frequent itemsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
    "\n",
    "# Display the association rules\n",
    "print(\"Association Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, after obtaining the frequent itemsets, the mlxtend library's association_rules function is used to generate association rules based on a confidence threshold. The output includes rules with information about antecedent (if) and consequent (then) itemsets, as well as metrics like support, confidence, and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Improving the efficiency of Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a powerful tool for finding frequent itemsets and generating association rules, but it can become computationally expensive, especially for large datasets. Several techniques can be applied to improve the efficiency of the Apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Strategies for Efficiency Improvement:\n",
    "\n",
    "1. Pruning Strategies:\n",
    "        Implementing strategies such as the \"hash-based\" or \"tree-based\" pruning can help reduce the search space by avoiding the generation of candidate itemsets that are known to be infrequent.\n",
    "\n",
    "2. Transaction Reduction:\n",
    "        Reducing the size of transactions or considering a smaller sample of transactions can significantly speed up the Apriori algorithm.\n",
    "\n",
    "3. Parallelization:\n",
    "        Distributing the workload across multiple processors or nodes can lead to a substantial improvement in performance, especially for datasets with a large number of transactions.\n",
    "\n",
    "4. Dynamic Itemset Counting:\n",
    "        Dynamically adjusting the minimum support threshold based on the size of the dataset or the progress of the algorithm can optimize the search for frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Efficiency Improvement in Apriori Algorithm in Python:\n",
    "\n",
    "Let's demonstrate a simple example of how to parallelize the Apriori algorithm using the mlxtend library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a large retail dataset\n",
    "data = {\n",
    "    'TransactionID': [i for i in range(1, 10001)],\n",
    "    'Item': ['A', 'B', 'C', 'D', 'E'] * 2000\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for parallelized Apriori algorithm\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply parallelized Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True, n_jobs=-1)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the n_jobs=-1 parameter in the apriori function instructs the algorithm to use all available processors for parallelization. This can significantly improve the efficiency of the Apriori algorithm, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. A pattern-growth approach for mining frequent itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pattern-growth approach is an alternative strategy for mining frequent itemsets that differs from the traditional Apriori algorithm. Instead of candidate generation and pruning, the pattern-growth approach grows frequent itemsets from smaller ones by leveraging a compact data structure called a prefix tree or FP-tree. This approach can be more memory-efficient and faster, particularly for datasets with a large number of transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Steps in a Pattern-Growth Approach:\n",
    "\n",
    "1. Constructing the FP-Tree:\n",
    "        Build a compact data structure, the FP-tree, from the transaction dataset. The FP-tree efficiently represents the frequency of itemsets.\n",
    "\n",
    "2. Mining Frequent Itemsets:\n",
    "        Use recursive techniques to mine frequent itemsets directly from the FP-tree without generating candidate itemsets explicitly.\n",
    "\n",
    "3. Conditional Pattern Base:\n",
    "        Utilize a conditional pattern base to find frequent itemsets in a more targeted and efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Pattern-Growth Approach in Python:\n",
    "\n",
    "Let's use the mlxtend library to implement the FP-growth algorithm on a sample retail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for FP-growth\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply FP-growth algorithm to find frequent itemsets\n",
    "frequent_itemsets = fpgrowth(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the FP-growth algorithm from the mlxtend library is applied to find frequent itemsets in a retail dataset. The fpgrowth function generates itemsets directly without the need for candidate generation and pruning, making it a pattern-growth approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5. Mining frequent itemsets using the vertical data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mining frequent itemsets can be approached using the vertical data format, an alternative representation of transactional data. In this format, each record corresponds to a unique item and its associated transactions. This approach can be particularly useful when dealing with high-dimensional datasets where the number of distinct items is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Vertical Data Format:\n",
    "    In the vertical data format, each item and its corresponding transactions are stored together. This format simplifies the process of identifying frequent itemsets by focusing on each item independently.\n",
    "\n",
    "2. Transaction Identifier Lists:\n",
    "    For each item, a list of transaction identifiers is maintained, indicating in which transactions the item occurs. This allows for efficient counting of support for each item.\n",
    "\n",
    "3. Inverse Transaction Mapping:\n",
    "    In addition to lists of transaction identifiers for each item, an inverse mapping is often used to retrieve the items associated with a given transaction efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Mining Frequent Itemsets in Vertical Data Format in Python:\n",
    "\n",
    "Let's use the mlxtend library to demonstrate mining frequent itemsets using the vertical data format on a sample retail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset in vertical format\n",
    "data = {\n",
    "    'TransactionID': [1, 2, 3, 4, 5],\n",
    "    'Items': [['A', 'B', 'D'], ['A', 'C', 'D'], ['B', 'D'], ['A', 'B', 'C'], ['A', 'C']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert data to the horizontal format using TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(df['Items']).transform(df['Items'])\n",
    "horizontal_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(horizontal_df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, the dataset is represented in the vertical format, and the TransactionEncoder from the mlxtend library is used to convert it to the horizontal format. The Apriori algorithm is then applied to find frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.6. Mining closed and max patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data mining, beyond finding frequent itemsets, there is interest in discovering closed and max patterns. These patterns provide more concise and specific information about frequent itemsets, aiding in the interpretation and application of mining results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Closed Patterns:\n",
    "    Closed patterns are frequent itemsets that cannot be extended by adding any more items without decreasing their support. These patterns offer a compact representation of frequent itemsets by eliminating redundant combinations.\n",
    "\n",
    "2. Max Patterns:\n",
    "    Max patterns are the largest frequent itemsets in terms of support and are not subsumed by any other frequent itemset. Max patterns provide a more condensed and focused view of the most significant patterns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Mining Closed and Max Patterns in Python:\n",
    "\n",
    "Let's use the mlxtend library to mine closed and max patterns from a sample retail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for FP-growth\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply FP-growth algorithm to find frequent itemsets\n",
    "frequent_itemsets = fpgrowth(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate closed patterns\n",
    "closed_patterns = frequent_itemsets[frequent_itemsets['support'] == frequent_itemsets['support']]\n",
    "\n",
    "# Generate max patterns\n",
    "max_patterns = frequent_itemsets.groupby('itemsets')['support'].idxmax()\n",
    "max_patterns = frequent_itemsets.loc[max_patterns]\n",
    "\n",
    "# Display the closed and max patterns\n",
    "print(\"Closed Patterns:\")\n",
    "print(closed_patterns)\n",
    "\n",
    "print(\"\\nMax Patterns:\")\n",
    "print(max_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, after finding frequent itemsets using the FP-growth algorithm, closed patterns are identified by selecting itemsets with support equal to their own support. Max patterns are determined by selecting the itemsets with the maximum support for each set of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Strong rules are not necessarily interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of association rule mining, it's important to understand that strong rules, determined by metrics like confidence and lift, are not always inherently interesting or valuable. While these metrics quantify the strength and significance of association rules, they may not capture the real-world relevance or usefulness of the discovered patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Confidence and Lift:\n",
    "\n",
    "    Confidence measures the likelihood that item Y is purchased given the purchase of item X.\n",
    "\n",
    "    Lift measures the strength of the association between items, considering the independence of X and Y.\n",
    "\n",
    "2. Interest and Context:\n",
    "\n",
    "    Strong rules may have high confidence and lift but lack practical significance or actionable insights in a specific context.\n",
    "\n",
    "    The interpretation and usefulness of rules depend on the domain and the goals of the data mining process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Strong Rules Not Being Interesting in Python:\n",
    "\n",
    "Let's use the mlxtend library to demonstrate the concept of strong rules that may not be interesting in a practical context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
    "\n",
    "# Display the association rules\n",
    "print(\"Association Rules:\")\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, association rules are generated using the Apriori algorithm, and rules with a confidence threshold of 0.7 are selected. However, it's crucial to emphasize to students that high confidence alone does not guarantee the real-world relevance or interestingness of a rule. The interpretation and application of rules should be considered in the specific context of the data and business domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. From association analysis to correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association analysis and correlation analysis are two related but distinct techniques in data mining. While association analysis focuses on discovering relationships between items or variables, correlation analysis quantifies the strength and direction of a linear relationship between two numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Association Analysis:\n",
    "\n",
    "    Association analysis is concerned with finding interesting patterns or relationships in large datasets.\n",
    "\n",
    "    It often deals with categorical data and aims to discover rules indicating co-occurrence or sequence patterns.\n",
    "\n",
    "2. Correlation Analysis:\n",
    "\n",
    "    Correlation analysis, on the other hand, is used for numeric variables.\n",
    "\n",
    "    It measures the strength and direction of a linear relationship between two variables, expressed as the correlation coefficient.\n",
    "\n",
    "3. Pearson Correlation Coefficient:\n",
    "\n",
    "    The Pearson correlation coefficient ranges from -1 to 1, where:\n",
    "\n",
    "        1 indicates a perfect positive linear relationship,\n",
    "\n",
    "        -1 indicates a perfect negative linear relationship, and\n",
    "        \n",
    "        0 indicates no linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Moving from Association Analysis to Correlation Analysis in Python:\n",
    "\n",
    "Let's use a dataset with numeric variables to illustrate the transition from association analysis to correlation analysis in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate a dataset with numeric variables\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [5, 4, 3, 2, 1],\n",
    "    'Feature3': [2, 4, 1, 5, 3]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Visualize the dataset using a heatmap\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, we create a dataset with three numeric features. We then use the Seaborn library to generate a heatmap illustrating the Pearson correlation coefficients between these features.\n",
    "\n",
    "    That while association analysis might involve discovering rules like \"if Feature1=3, then Feature2=3,\" correlation analysis quantifies the degree and direction of the linear relationship between Feature1 and Feature2. Emphasize the interpretation of correlation coefficients and how they can provide insights into the strength and nature of relationships between numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. A comparison of pattern evaluation measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm writing notebooks for Data Mining concept and techniques as teacher assistant. Help me with a short explanation on \n",
    "4.3.3. A comparison of pattern evaluation measures\n",
    "And a real world example of practical use in python for it to put in notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts:\n",
    "\n",
    "1. Support:\n",
    "    Support measures the frequency of occurrence of a pattern in the dataset. It is the proportion of transactions that contain the pattern.\n",
    "\n",
    "2. Confidence:\n",
    "    Confidence quantifies the strength of an association rule. It is the conditional probability of the consequent given the antecedent.\n",
    "\n",
    "3. Lift:\n",
    "    Lift assesses the significance of an association rule by comparing the observed support of the rule to what would be expected if the antecedent and consequent were independent.\n",
    "\n",
    "4. Conviction:\n",
    "    Conviction measures the degree of implication between the antecedent and consequent in a rule. It indicates how much more likely the consequent is if the antecedent is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Comparing Pattern Evaluation Measures in Python:\n",
    "\n",
    "Let's use a sample retail dataset to demonstrate the calculation of support, confidence, lift, and conviction for association rules in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the mlxtend library if not already installed\n",
    "# pip install mlxtend\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Simulate a retail dataset\n",
    "data = {\n",
    "    'TransactionID': [1, 1, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Item': ['A', 'B', 'A', 'B', 'A', 'B', 'C', 'A', 'B', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding to prepare data for Apriori algorithm\n",
    "basket = pd.crosstab(index=df['TransactionID'], columns=df['Item']).applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(basket, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
    "\n",
    "# Display the rules with support, confidence, lift, and conviction\n",
    "print(\"Association Rules with Evaluation Measures:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'conviction']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this example, association rules are generated using the Apriori algorithm, and evaluation measures such as support, confidence, lift, and conviction are calculated for each rule. This allows for a comparison of the different measures and their implications for rule quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining_2024_spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
